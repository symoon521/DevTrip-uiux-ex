

# **쿠버네티스 마스터 가이드: 개념부터 운영까지**

## **파트 1: 쿠버네티스 기초**

### **섹션 1: 쿠버네티스 소개**

이 섹션에서는 쿠버네티스의 근본적인 배경을 다룹니다. 쿠버네티스가 무엇인지 설명하기에 앞서, 왜 필요한지에 대한 맥락을 이해하고, 애플리케이션 배포 방식의 진화와 컨테이너 오케스트레이션의 등장을 살펴봅니다.

#### **1.1 컨테이너 오케스트레이션이란 무엇인가? 자동화된 관리의 필요성**

컨테이너화(Containerization)는 애플리케이션과 그에 필요한 모든 종속성을 하나의 패키지로 묶는 소프트웨어 배포 방식입니다.1 단일 컨테이너화된 애플리케이션을 관리하는 것은 비교적 간단하지만, 분산 시스템 전반에 걸쳐 수백, 수천 개의 컨테이너로 확장하는 것은 가용성, 상호 연결성, 확장성 측면에서 상당한 어려움을 야기합니다.1

이러한 문제를 해결하기 위해 \*\*컨테이너 오케스트레이션(Container Orchestration)\*\*이 등장했습니다. 이는 컨테이너의 배포, 관리, 확장, 네트워킹을 자동화하는 시스템을 의미합니다.3 오케스트레이션 도구는 여러 인스턴스에 걸쳐 컨테이너를 관리하고, 다양한 애플리케이션을 실행하며, 장애 발생 시 서비스 연속성을 보장하고, 리소스 활용도를 극대화해야 할 때 필수적입니다.4

컨테이너 오케스트레이션의 핵심적인 가치는 단순한 자동화를 넘어섭니다. 초기 컨테이너 관리는 수동 스크립팅과 "이것을 하고, 그다음 저것을 하라"는 식의 명령형(Imperative) 접근 방식에 의존했습니다.4 이는 대규모 환경에서는 복잡하고 오류가 발생하기 쉬웠습니다. 반면, 쿠버네티스와 같은 현대적인 오케스트레이션 플랫폼은 "내가 원하는 최종 상태는 이것이다"라고 정의하는 선언형(Declarative) 모델을 도입했습니다.2 사용자는 YAML과 같은 매니페스트 파일에 최종 상태를 정의하고, 오케스트레이터는 지속적으로 현재 상태를 원하는 상태와 일치시키기 위해 작동합니다.6

이러한 패러다임의 전환은 단순한 편의성을 넘어, 시스템이 스스로 상태를 복구하는 **자체 복구(Self-healing)** 7와

**자동화된 롤아웃 및 롤백(Automated rollouts and rollbacks)** 7을 가능하게 하는 근본적인 변화입니다. 시스템 자체가 상태를 조정할 책임을 지게 되면서, 운영자의 개입 없이도 높은 회복탄력성을 유지할 수 있게 됩니다. 따라서 컨테이너 오케스트레이션의 진정한 가치는 인프라 관리를 위한 선언적이고 자체 복구적인 모델을 채택하는 데 있으며, 이는 현대 DevOps 및 클라우드 네이티브 방식의 초석이 됩니다.

#### **1.2 쿠버네티스 정의: 사실상의 표준**

**쿠버네티스(Kubernetes)**, 또는 **K8s**는 컨테이너화된 애플리케이션의 배포, 확장 및 관리를 자동화하기 위한 오픈소스 시스템입니다.7 쿠버네티스는 애플리케이션을 구성하는 컨테이너들을 \*\*파드(Pod)\*\*라는 논리적 단위로 그룹화하여 관리와 검색을 용이하게 합니다.7 리눅스 다음으로 세계에서 두 번째로 큰 오픈소스 프로젝트이며, 포춘 100대 기업의 대다수가 사용하는 컨테이너 오케스트레이션의 사실상 표준(de facto standard)으로 자리 잡았습니다.9

쿠버네티스의 핵심 기능은 다음과 같습니다 7:

* **자동화된 롤아웃 및 롤백 (Automated rollouts and rollbacks)**: 애플리케이션 상태를 모니터링하며 점진적으로 변경 사항을 배포하고, 문제가 발생하면 자동으로 변경 사항을 되돌립니다.  
* **서비스 디스커버리 및 로드 밸런싱 (Service discovery and load balancing)**: 자체 IP 주소와 단일 DNS 이름을 사용하여 파드 집합을 노출하고, 이들 간의 트래픽을 분산시킵니다.  
* **스토리지 오케스트레이션 (Storage orchestration)**: 로컬 스토리지, 퍼블릭 클라우드 공급자 또는 NFS나 iSCSI와 같은 네트워크 스토리지 시스템 등 원하는 스토리지 시스템을 자동으로 마운트합니다.  
* **시크릿 및 구성 관리 (Secret and configuration management)**: 이미지를 재빌드하지 않고도 시크릿과 애플리케이션 구성을 배포하고 업데이트할 수 있습니다.  
* **자동화된 빈 패킹 (Automatic bin packing)**: 가용성을 희생하지 않으면서 리소스 요구사항 및 기타 제약 조건에 따라 컨테이너를 자동으로 배치하여 리소스 활용도를 극대화합니다.  
* **자체 복구 (Self-healing)**: 실패한 컨테이너를 다시 시작하고, 파드를 교체하며, 응답하지 않는 노드의 워크로드를 재스케줄링합니다.  
* **수평적 확장 (Horizontal scaling)**: 간단한 명령, UI 또는 CPU 사용량에 따라 자동으로 애플리케이션을 확장하거나 축소합니다.

#### **1.3 간략한 역사: 구글의 Borg에서 CNCF까지**

쿠버네티스의 역사는 2000년대 초반 구글이 개발한 내부 클러스터 관리 시스템인 **Borg**에서 시작됩니다.9 Borg는 구글의 대규모 클러스터에서 수십만 개의 작업을 관리하며, 훗날 쿠버네티스에 채택될 파드, 서비스, 레이블과 같은 개념들을 개척했습니다.10 이후 구글은 더 유연한 후속 시스템으로

**Omega**를 개발했습니다.10

2013년, 구글의 엔지니어인 조 베다(Joe Beda), 브렌던 번스(Brendan Burns), 크레이그 맥러키(Craig McLuckie)는 "프로젝트 7(Project 7)"이라는 코드명으로 새로운 시스템 개발을 시작했습니다. 이는 스타트렉의 전직 Borg 캐릭터인 '세븐 오브 나인(Seven of Nine)'에 대한 경의의 표시였습니다.9 이 프로젝트가 바로 쿠버네티스가 되었으며, C++로 작성된 Borg와 달리 Go 언어로 개발되었습니다.13

쿠버네티스는 2014년 6월에 발표되었고, 2015년 7월에 버전 1.0이 출시되었습니다.13 여기서 구글은 중대한 전략적 결정을 내립니다. 당시 Docker가 컨테이너 기술을 대중화했지만, 강력한 다중 노드 오케스트레이션 솔루션은 부재한 상황이었습니다.9 구글은 당시 클라우드 시장의 지배적 사업자인 Amazon Web Services(AWS)와 경쟁할 방법을 모색하고 있었습니다.9 Borg와 같은 독점적인 오케스트레이터를 자사 클라우드에서만 제공하는 것은 시장의 매력을 얻기 어렵고 벤더 종속성에 대한 우려를 낳을 수 있었습니다.

대신 구글은 쿠버네티스를 오픈소스로 공개하고, 리눅스 재단 산하에 새로 설립된 중립적인 기관인 \*\*클라우드 네이티브 컴퓨팅 재단(Cloud Native Computing Foundation, CNCF)\*\*에 기부했습니다.9 이 결정은 쿠버네티스가 구글의 제품이 아닌 산업 표준으로 자리매김하게 만들었으며, 마이크로소프트, 레드햇, IBM과 같은 경쟁사들의 참여와 기여를 이끌어냈습니다.9 이 전략 덕분에 쿠버네티스는 "어디서든 실행(Run Anywhere)" 7될 수 있는 공통 추상화 계층이 되었고, 이는 오케스트레이션 계층을 상용화하여 경쟁의 장을 GKE, EKS, AKS와 같은

*관리형* 쿠버네티스 서비스의 품질로 옮겨 놓았습니다. 결국, 쿠버네티스를 오픈소스로 공개한 것은 구글의 10년이 넘는 운영 경험 7을 활용하여 기술 산업 전체의 표준을 만들고, 단일 클라우드 제공업체가 컨테이너 생태계를 독점하는 것을 방지한 탁월한 전략이었습니다.

#### **1.4 쿠버네티스의 핵심 이점**

쿠버네티스는 현대적인 애플리케이션 배포 및 관리에 있어 다음과 같은 핵심적인 이점을 제공합니다.

* **확장성 (Scalability)**: 수요에 따라 애플리케이션을 자동으로 확장하거나 축소할 수 있습니다.7 이는 구글의 행성 규모(planet-scale) 운영을 위해 설계된 Borg의 유산입니다.7  
* **고가용성 및 자체 복구 (High Availability & Self-Healing)**: 실패한 컨테이너를 자동으로 재시작하고, 파드를 교체하며, 실패한 노드의 워크로드를 재스케줄링하여 다운타임을 최소화합니다.7  
* **이식성 및 일관성 (Portability & Consistency)**: 개발 환경부터 프로덕션 환경까지, 그리고 온프레미스, 하이브리드, 퍼블릭 클라우드 인프라 전반에 걸쳐 일관된 애플리케이션 환경을 제공합니다.1 이는 "내 컴퓨터에서는 잘 됐는데"라는 고질적인 문제를 해결합니다.  
* **비용 효율성 및 리소스 최적화 (Cost-effectiveness & Resource Optimization)**: 자동화된 빈 패킹은 리소스 요구사항에 따라 컨테이너를 배치하여 활용도를 높이고 자원을 절약합니다.2 또한, 컨테이너 자체는 가상 머신(VM)보다 가벼워 오버헤드를 더욱 줄여줍니다.2

### **섹션 2: 쿠버네티스 클러스터 아키텍처**

이 섹션에서는 쿠버네티스 클러스터를 구성하는 요소들을 해부하고, 컨트롤 플레인과 워커 노드의 각 부분이 어떤 역할을 하며 어떻게 상호작용하는지 설명합니다.

#### **2.1 고수준 개요: 컨트롤 플레인과 워커 노드**

쿠버네티스 클러스터는 마스터/워커(또는 클라이언트-서버) 아키텍처를 따릅니다.13 클러스터는 크게 두 부분으로 구성됩니다: 클러스터의 '두뇌' 역할을 하는 \*\*컨트롤 플레인(Control Plane)\*\*과 애플리케이션이 실제로 실행되는 '근육' 역할을 하는 하나 이상의 \*\*워커 노드(Worker Nodes)\*\*입니다.6

컨트롤 플레인은 스케줄링과 같은 클러스터에 대한 전반적인 결정을 내리고, 사용자가 정의한 \*원하는 상태(desired state)\*를 유지하는 역할을 합니다. 워커 노드는 실제 컨테이너화된 워크로드를 실행하는 물리적 또는 가상 머신입니다.6 이들 간의 모든 통신은 컨트롤 플레인의 API 서버를 통해 관리됩니다.15

#### **2.2 컨트롤 플레인 심층 분석**

컨트롤 플레인은 쿠버네티스 클러스터의 모든 활동을 조정하고 관리하는 핵심 구성 요소들로 이루어져 있습니다.

* **API 서버 (kube-apiserver)**: 컨트롤 플레인의 프론트엔드로서, 쿠버네티스 REST API를 노출합니다.18 이는 클러스터 내외부의 모든 상호작용을 위한 중앙 통신 허브 역할을 합니다.17 API 서버는 요청을 검증하고, 인증 및 권한 부여를 처리하며, 클러스터의 상태를 저장하는  
  etcd와 직접 통신하는 유일한 컴포넌트입니다.17  
* **etcd**: 클러스터의 '데이터베이스' 또는 '진실의 원천(source of truth)' 역할을 하는 일관성 있고 고가용성을 갖춘 분산 키-값 저장소입니다.6  
  etcd는 파드, 시크릿, 디플로이먼트 등 모든 쿠버네티스 객체의 구성, 상태, 메타데이터를 저장합니다.24  
  etcd의 안정성은 클러스터의 안정성과 직결되며, etcd 데이터의 손실은 클러스터 상태의 손실을 의미합니다.18 데이터 일관성을 보장하기 위해 Raft 합의 알고리즘을 사용합니다.24  
* **스케줄러 (kube-scheduler)**: 아직 노드가 할당되지 않은 새로 생성된 파드를 감시하고, 이들을 실행할 최적의 워커 노드를 선택하는 역할을 합니다.6 스케줄러의 결정 과정은 두 단계로 이루어집니다:  
  1. **필터링(Filtering)**: 파드의 요구사항(예: CPU, 메모리)을 충족하는 실행 가능한 노드들의 목록을 만듭니다.26  
  2. **스코어링(Scoring)**: 필터링된 노드들의 순위를 매겨 가장 적합한 노드를 선택합니다.26

     이 과정에서 리소스 요구사항, 제약 조건, 어피니티/안티-어피니티 규칙, 정책 등 다양한 요소가 고려됩니다.28  
* **컨트롤러 매니저 (kube-controller-manager)**: 여러 컨트롤러 프로세스를 실행하는 데몬으로, 지속적인 \*\*조정 루프(reconciliation loop)\*\*를 통해 클러스터를 관리합니다.6 각 컨트롤러는 API 서버를 통해 클러스터의 현재 상태를 감시하고, 원하는 상태와 달라진 부분이 있으면 이를 바로잡기 위한 조치를 취합니다.30 주요 컨트롤러는 다음과 같습니다:  
  * **노드 컨트롤러 (Node Controller)**: 노드의 상태를 모니터링하고 장애 발생 시 조치를 취합니다.  
  * **복제 컨트롤러 (Replication Controller)**: 지정된 수의 파드 복제본이 항상 실행되도록 보장합니다.  
  * **엔드포인트 컨트롤러 (Endpoints Controller)**: 서비스(Service)와 파드를 연결하는 엔드포인트 객체를 채웁니다.6  
* **클라우드 컨트롤러 매니저 (cloud-controller-manager)**: (선택 사항) 클라우드 제공업체별 제어 로직을 포함하는 컴포넌트입니다.17 이를 통해 쿠버네티스는 기본 클라우드 플랫폼의 API와 상호작용하여 클라우드 로드 밸런서나 스토리지 볼륨과 같은 리소스를 관리할 수 있습니다. 이는 쿠버네티스 핵심 코드와 특정 클라우드 제공업체의 구현을 분리하는 역할을 합니다.17

#### **2.3 워커 노드 심층 분석**

워커 노드는 컨트롤 플레인으로부터 명령을 받아 실제 컨테이너화된 애플리케이션을 실행하는 역할을 합니다. 모든 워커 노드에는 다음과 같은 컴포넌트가 실행됩니다.

* **Kubelet**: 각 워커 노드에서 실행되는 기본 노드 에이전트입니다.19 컨트롤 플레인의 API 서버와 통신하며, \*\*파드 명세(PodSpec)\*\*에 정의된 컨테이너들이 해당 노드에서 건강하게 실행되도록 보장합니다.18 Kubelet은 이미지 가져오기, 컨테이너 시작/중지 등 파드의 전체 생명주기를 관리하고, 노드와 파드의 상태를 컨트롤 플레인에 보고합니다.19  
* **Kube-proxy**: 각 노드에서 실행되는 네트워크 프록시로, 노드의 네트워크 규칙을 유지 관리합니다.19 이는 쿠버네티스  
  **서비스(Service)** 개념을 구현하여, 클러스터 내부 또는 외부에서 파드로의 네트워크 통신을 가능하게 합니다. kube-proxy는 iptables나 IPVS와 같은 호스트의 네트워킹 기능을 사용하여 트래픽을 적절한 파드로 전달합니다.17  
* **컨테이너 런타임 (Container Runtime)**: 컨테이너를 실행하는 역할을 하는 소프트웨어입니다.15 쿠버네티스는 \*\*컨테이너 런타임 인터페이스(Container Runtime Interface, CRI)\*\*를 통해 Docker, containerd, CRI-O 등 다양한 런타임을 지원합니다.17 Kubelet은 컨테이너 런타임과 상호작용하여 컨테이너의 생명주기를 관리합니다.35

쿠버네티스의 아키텍처는 단순히 컴포넌트들이 서로 명령을 주고받는 시스템이 아닙니다. 스케줄러, 컨트롤러 매니저, Kubelet 등 모든 주요 컴포넌트는 API 서버를 \*감시(watch)\*하며 상태 변화를 인지합니다.17 이들은 직접적인 명령을 받지 않습니다. 컨트롤러 매니저는 '조정 루프'를 실행하며, API 서버를 통해

etcd에 저장된 *원하는 상태*와 *현재 상태*를 비교하고 그 차이를 줄이기 위해 행동합니다.30

이 원칙은 시스템 전체에 적용됩니다. 스케줄러는 스케줄링되지 않은 파드를 감시하고, Kubelet은 자신의 노드에 할당된 파드를 감시하며, Kube-proxy는 서비스와 엔드포인트의 변경을 감시합니다. Borg와 Omega의 동기적이고 모놀리식한 설계에서 얻은 교훈을 바탕으로 한 이 아키텍처는 11 시스템을 놀랍도록 회복탄력적으로 만듭니다. 만약 어떤 컴포넌트가 실패하고 재시작되더라도, 그저 API 서버를 다시 감시하고 중단된 지점부터 상태 조정을 재개하면 됩니다. 컴포넌트 자체에는 복구해야 할 복잡한 상태가 없으며, '진실'은 항상

etcd에 있습니다. 이 API 중심의 조정 루프 기반 아키텍처가 바로 쿠버네티스의 회복탄력성, 확장성, 그리고 자체 복구 능력의 비결입니다. 이를 이해하는 것이 쿠버네티스의 진정한 작동 방식을 이해하는 열쇠입니다.

| 컴포넌트 분류 | 컴포넌트 이름 | 주요 역할 | 핵심 상호작용 |
| :---- | :---- | :---- | :---- |
| **컨트롤 플레인** | kube-apiserver | 클러스터의 모든 상호작용을 위한 중앙 API 엔드포인트를 제공합니다. | etcd와 직접 통신하며, 다른 모든 컴포넌트로부터의 요청을 처리합니다. |
|  | etcd | 클러스터의 모든 구성과 상태 데이터를 저장하는 키-값 저장소입니다. | API 서버의 유일한 데이터 저장소 역할을 합니다. |
|  | kube-scheduler | 새로 생성된 파드를 실행할 최적의 워커 노드를 선택합니다. | API 서버를 감시하여 스케줄링되지 않은 파드를 찾아 노드에 할당합니다. |
|  | kube-controller-manager | 클러스터의 상태를 원하는 상태로 유지하기 위해 다양한 컨트롤러를 실행합니다. | API 서버를 감시하여 리소스 상태를 조정합니다 (예: 파드 복제본 수 유지). |
|  | cloud-controller-manager | 클라우드 제공업체별 리소스(로드 밸런서, 스토리지)를 관리합니다. | 클라우드 제공업체의 API와 통신하여 클러스터 리소스를 관리합니다. |
| **워커 노드** | kubelet | 각 노드에서 파드 내 컨테이너가 실행되고 건강한지 확인하는 에이전트입니다. | API 서버로부터 파드 명세를 받아 컨테이너 런타임을 통해 컨테이너를 관리합니다. |
|  | kube-proxy | 각 노드의 네트워크 규칙을 관리하여 서비스로의 트래픽 라우팅을 담당합니다. | API 서버를 감시하여 서비스 및 엔드포인트 변경에 따라 네트워크 규칙을 업데이트합니다. |
|  | Container Runtime | 컨테이너를 실제로 실행하는 소프트웨어입니다. | Kubelet의 지시에 따라 컨테이너 이미지를 가져오고 컨테이너를 실행합니다. |

---

## **파트 2: 쿠버네티스 핵심 오브젝트와 개념**

이 파트에서는 아키텍처에서 벗어나 개발자와 운영자가 매일 상호작용하는 실질적인 API 오브젝트들을 다룹니다. 각 섹션에는 상세한 주석이 달린 YAML 예제가 포함됩니다.

### **섹션 3: 워크로드 관리**

#### **3.1 파드(Pod): 가장 작은 배포 단위**

파드는 쿠버네티스에서 생성하고 관리할 수 있는 가장 작고 간단한 배포 단위로, 실행 중인 프로세스의 단일 인스턴스를 나타냅니다.2 파드는 하나 이상의 컨테이너를 포함하며, 이 컨테이너들은 스토리지, 고유한 네트워크 IP 주소, 그리고 기타 리소스를 공유합니다.6 파드는 영구적이지 않으며 교체 가능하도록 설계되었습니다.6 이 개념은 구글 Borg의 "alloc" 12과 Omega의 "SUnit" 36에서 발전했습니다.

파드의 생명주기는 Pending(생성 수락, 스케줄링 대기), Running(노드에 바인딩, 컨테이너 실행 중), Succeeded(모든 컨테이너 성공적으로 종료), Failed(하나 이상의 컨테이너 실패로 종료), Unknown(상태 확인 불가) 등의 단계를 거칩니다.

**단일 컨테이너 파드 YAML 예시**

다음은 단일 Nginx 컨테이너를 실행하는 기본적인 파드의 YAML 정의입니다.

YAML

\# pod-single-container.yaml  
apiVersion: v1  
kind: Pod  
metadata:  
  name: nginx-pod \# 파드의 고유한 이름  
  labels:  
    app: webserver \# 파드를 식별하고 그룹화하기 위한 레이블  
spec:  
  containers:  
    \- name: nginx-container \# 파드 내 컨테이너의 이름  
      image: nginx:1.25 \# 컨테이너를 생성할 도커 이미지  
      ports:  
        \- containerPort: 80 \# 컨테이너가 노출하는 포트

**다중 컨테이너 파드 YAML 예시 (사이드카 패턴)**

하나의 파드에 여러 컨테이너를 배치하는 것은 강력한 패턴입니다. **사이드카(Sidecar)** 패턴은 주 애플리케이션 컨테이너의 기능을 확장하거나 보조하는 헬퍼 컨테이너를 추가하는 방식입니다. 예를 들어, 주 애플리케이션의 로그를 수집하여 중앙 로깅 시스템으로 전송하는 로깅 에이전트를 사이드카로 추가할 수 있습니다. 파드 내의 컨테이너들은 동일한 네트워크 네임스페이스를 공유하므로 localhost를 통해 통신할 수 있습니다.12

다음 예시는 Nginx 웹 서버(주 컨테이너)와, busybox를 사용하여 10초마다 Nginx의 index.html 파일을 요청하고 그 결과를 로그로 남기는 사이드카 컨테이너를 포함합니다.

YAML

\# pod-multi-container-sidecar.yaml  
apiVersion: v1  
kind: Pod  
metadata:  
  name: nginx-with-sidecar  
spec:  
  containers:  
    \# 주 애플리케이션 컨테이너  
    \- name: main-nginx  
      image: nginx:1.25  
      ports:  
        \- containerPort: 80  
      \# 공유 볼륨을 마운트하여 사이드카와 파일 공유  
      volumeMounts:  
        \- name: nginx-logs  
          mountPath: /var/log/nginx

    \# 사이드카 컨테이너  
    \- name: sidecar-logger  
      image: busybox:1.36  
      \# 10초마다 localhost의 Nginx에 요청을 보내고 로그를 남기는 명령  
      command:  
      \# 공유 볼륨 마운트  
      volumeMounts:  
        \- name: nginx-logs  
          mountPath: /var/log/nginx  
    
  \# 컨테이너 간 파일 공유를 위한 볼륨 정의  
  volumes:  
    \- name: nginx-logs  
      emptyDir: {} \# 파드가 살아있는 동안 유지되는 임시 디렉토리

#### **3.2 디플로이먼트(Deployment): 상태 없는 애플리케이션 관리**

디플로이먼트는 파드와 레플리카셋(ReplicaSet)에 대한 선언적 업데이트를 제공하는 상위 수준의 리소스입니다.37 디플로이먼트는 동일한 파드 집합을 관리하며, 지정된 수의 복제본이 항상 실행되도록 보장하고 실패한 파드를 자동으로 교체합니다.37

디플로이먼트의 핵심 기능은 애플리케이션의 생명주기를 관리하는 것입니다. 주요 기능은 다음과 같습니다.

* **롤링 업데이트 (Rolling Updates)**: 이전 버전의 파드를 새로운 버전의 파드로 점진적으로 교체하여 무중단으로 애플리케이션을 업데이트합니다.38 업데이트 전략은  
  maxUnavailable(업데이트 중 사용할 수 없는 최대 파드 수)과 maxSurge(원하는 복제본 수를 초과하여 생성할 수 있는 최대 파드 수) 옵션으로 세밀하게 조정할 수 있습니다.39  
* **롤백 (Rollbacks)**: 업데이트에 문제가 발생할 경우, kubectl rollout undo 명령어를 사용하여 이전의 안정적인 버전으로 쉽게 되돌릴 수 있습니다.38

**디플로이먼트 YAML 예시**

다음은 3개의 Nginx 파드 복제본을 생성하고 관리하는 디플로이먼트의 YAML 정의입니다.38

YAML

\# deployment.yaml  
apiVersion: apps/v1  
kind: Deployment  
metadata:  
  name: nginx-deployment \# 디플로이먼트의 이름  
  labels:  
    app: nginx  
spec:  
  replicas: 3 \# 실행할 파드의 복제본 수  
  selector:  
    matchLabels:  
      app: nginx \# 이 디플로이먼트가 관리할 파드를 선택하기 위한 레이블 셀렉터  
  template: \# 파드를 생성할 때 사용할 템플릿  
    metadata:  
      labels:  
        app: nginx \# 파드에 적용될 레이블. 위의 selector와 일치해야 함  
    spec:  
      containers:  
        \- name: nginx  
          image: nginx:1.25  
          ports:  
            \- containerPort: 80

#### **3.3 레플리카셋(ReplicaSet): 복제의 기반**

레플리카셋의 목적은 특정 시점에 안정적인 수의 파드 복제본이 실행되도록 유지하는 것입니다.41 지정된 수의 동일한 파드가 항상 사용 가능하도록 보장합니다.42

일반적으로 사용자가 레플리카셋을 직접 관리할 필요는 없습니다. 대신, 디플로이먼트가 레플리카셋을 관리합니다.42 롤링 업데이트 중에 디플로이먼트는 애플리케이션의 새 버전에 대한 새로운 레플리카셋을 생성하고, 이전 레플리카셋의 파드 수를 점차 줄여나갑니다. 이 메커니즘 덕분에 버전 관리와 롤백이 가능해집니다.

다음은 3개의 Nginx 파드를 보장하는 레플리카셋의 간단한 예시입니다.

YAML

\# replicaset.yaml  
apiVersion: apps/v1  
kind: ReplicaSet  
metadata:  
  name: nginx-replicaset  
spec:  
  replicas: 3  
  selector:  
    matchLabels:  
      app: nginx-rs  
  template:  
    metadata:  
      labels:  
        app: nginx-rs  
    spec:  
      containers:  
        \- name: nginx  
          image: nginx:1.25

#### **3.4 스테이트풀셋(StatefulSet): 상태 저장 애플리케이션 처리**

스테이트풀셋은 데이터베이스와 같이 상태를 저장해야 하는 워크로드를 위해 특별히 설계된 컨트롤러입니다.43 파드를 교체 가능한 자원으로 취급하는 디플로이먼트와 달리, 스테이트풀셋은 각 파드에 안정적이고 고유하며 영구적인 식별자를 제공합니다.43

스테이트풀셋을 구별하는 주요 특징은 다음과 같습니다.

* **안정적인 고유 네트워크 식별자**: 파드는 web-0, web-1과 같이 예측 가능한 순서 인덱스를 가진 이름과 안정적인 호스트 이름을 부여받습니다.43 이는 상태 저장 시스템에서 서비스 디스커버리에 매우 중요하며, \*\*헤드리스 서비스(Headless Service)\*\*를 통해 구현됩니다.43  
* **순차적 배포 및 스케일링**: 파드는 생성, 확장, 종료 시 엄격하고 예측 가능한 순서(생성 시 0부터 N-1, 종료 시 N-1부터 0)를 따릅니다.44 이는 시작/종료 순서에 의존성이 있는 애플리케이션에 필수적입니다.  
* **영구 스토리지**: 각 파드는 volumeClaimTemplates 섹션을 통해 자신만의 고유한 영구 볼륨 클레임(PersistentVolumeClaim)을 할당받습니다. 이를 통해 파드가 다른 노드로 재스케줄링되더라도 데이터가 보존됩니다.43

**스테이트풀셋 YAML 예시**

다음은 Redis 데이터베이스를 스테이트풀셋으로 배포하는 전체 예시입니다. 여기에는 필수적인 헤드리스 서비스와 volumeClaimTemplates가 포함되어 있습니다.

YAML

\# statefulset-redis.yaml  
\# 1\. Headless Service 정의  
apiVersion: v1  
kind: Service  
metadata:  
  name: redis-headless  
  labels:  
    app: redis  
spec:  
  ports:  
    \- port: 6379  
      name: redis  
  clusterIP: None \# 헤드리스 서비스로 만들기 위해 clusterIP를 None으로 설정  
  selector:  
    app: redis \# 이 레이블을 가진 파드를 서비스의 엔드포인트로 선택

\---  
\# 2\. StatefulSet 정의  
apiVersion: apps/v1  
kind: StatefulSet  
metadata:  
  name: redis-sts  
spec:  
  serviceName: "redis-headless" \# 파드의 안정적인 네트워크 ID를 제공할 헤드리스 서비스 이름  
  replicas: 3 \# 3개의 Redis 인스턴스 (마스터 1, 레플리카 2\)  
  selector:  
    matchLabels:  
      app: redis  
  template:  
    metadata:  
      labels:  
        app: redis  
    spec:  
      containers:  
        \- name: redis  
          image: redis:7.2-alpine  
          ports:  
            \- containerPort: 6379  
              name: redis  
          volumeMounts:  
            \- name: redis-data \# 아래 volumeClaimTemplates의 이름과 일치  
              mountPath: /data  
    
  \# 3\. 각 파드에 대한 영구 스토리지를 동적으로 생성하기 위한 템플릿  
  volumeClaimTemplates:  
    \- metadata:  
        name: redis-data  
      spec:  
        accessModes: \# 단일 노드에서만 읽기/쓰기 가능  
        storageClassName: "standard" \# 사용할 스토리지 클래스 (사전 정의 필요)  
        resources:  
          requests:  
            storage: 1Gi \# 각 파드에 1GiB의 스토리지 요청

#### **3.5 데몬셋(DaemonSet): 모든 노드에 파드 실행 보장**

데몬셋은 클러스터의 모든 노드(또는 일부 노드)에 파드의 복사본이 하나씩 실행되도록 보장합니다.47 노드가 클러스터에 추가되면 해당 파드가 자동으로 추가되고, 노드가 제거되면 파드도 정리됩니다.47

이는 노드-로컬 기능을 제공하는 데 사용됩니다. 일반적인 사용 사례는 다음과 같습니다 47:

* **로그 수집 에이전트**: Fluentd나 Logstash 같은 에이전트를 모든 노드에 배포하여 로그를 수집합니다.  
* **노드 모니터링 에이전트**: Prometheus Node Exporter 같은 에이전트를 배포하여 각 노드의 성능 메트릭을 수집합니다.  
* **클러스터 스토리지 데몬**: Ceph나 GlusterFS 같은 스토리지 시스템의 데몬을 각 노드에서 실행합니다.

**데몬셋 YAML 예시**

다음은 모든 노드에 Prometheus Node Exporter를 배포하여 노드 메트릭을 수집하는 데몬셋의 예시입니다.

YAML

\# daemonset-node-exporter.yaml  
apiVersion: apps/v1  
kind: DaemonSet  
metadata:  
  name: node-exporter  
  labels:  
    app: node-exporter  
spec:  
  selector:  
    matchLabels:  
      app: node-exporter  
  template:  
    metadata:  
      labels:  
        app: node-exporter  
    spec:  
      \# 컨트롤 플레인 노드에서도 실행되도록 허용  
      tolerations:  
      \- effect: NoSchedule  
        key: node-role.kubernetes.io/control-plane  
      containers:  
      \- name: node-exporter  
        image: prom/node-exporter:v1.7.0  
        ports:  
        \- containerPort: 9100  
          protocol: TCP  
          name: http  
      \# 호스트의 네트워크를 직접 사용하여 노드 IP로 접근 가능하게 함  
      hostNetwork: true  
      hostPID: true

| 컨트롤러 | 주요 사용 사례 | 파드 식별자 | 스케일링 방식 | 스토리지 | 업데이트 전략 |
| :---- | :---- | :---- | :---- | :---- | :---- |
| **Deployment** | 상태 없는 웹 서버, API 등 | 임시적/교체 가능 | 임의의 복제본 수 | 공유 영구 볼륨 | 롤링 업데이트 |
| **StatefulSet** | 데이터베이스, 메시지 큐 등 상태 저장 앱 | 안정적/고유 | 순차적 스케일링 | 파드별 고유 영구 볼륨 | 순차적 업데이트 |
| **DaemonSet** | 로그 수집, 모니터링 등 노드 수준 에이전트 | 해당 없음 | 노드당 1개 파드 | HostPath 또는 노드-로컬 | 롤링 업데이트 |

### **섹션 4: 구성, 스토리지, 보안**

이 섹션에서는 애플리케이션 구성, 데이터 영속성, 접근 제어를 관리하는 데 필수적인 오브젝트들을 다룹니다.

#### **4.1 ConfigMap과 Secret: 구성 분리**

**ConfigMap**은 기밀이 아닌 구성 데이터를 키-값 쌍으로 저장하는 데 사용됩니다.49

**Secret**은 유사하지만 비밀번호, 토큰, API 키와 같은 소량의 민감한 데이터를 위해 특별히 설계되었습니다.50 핵심 원칙은 애플리케이션 코드로부터 구성을 분리하여 애플리케이션의 이식성을 높이는 것입니다.49

이 데이터는 세 가지 주요 방식으로 파드에 주입될 수 있습니다:

1. 환경 변수(Environment variables)  
2. 컨테이너의 커맨드 라인 인수(Command-line arguments)  
3. 볼륨의 파일(Files in a volume)

중요한 점은 쿠버네티스 Secret이 기본적으로 etcd에 저장될 때 base64로 인코딩만 될 뿐, 암호화되지는 않는다는 것입니다. 따라서 저장 데이터 암호화(encryption at rest)를 활성화하고, RBAC을 사용하여 접근을 제한하는 등의 보안 모범 사례를 따르는 것이 중요합니다.50

**ConfigMap 및 Secret 사용 예시**

먼저, ConfigMap과 Secret을 생성합니다.

YAML

\# configmap-secret.yaml  
\# 1\. ConfigMap 생성  
apiVersion: v1  
kind: ConfigMap  
metadata:  
  name: app-config  
data:  
  APP\_COLOR: "blue"  
  APP\_MODE: "production"  
\---  
\# 2\. Secret 생성 (값은 base64 인코딩 필요)  
apiVersion: v1  
kind: Secret  
metadata:  
  name: app-secret  
type: Opaque  
data:  
  \# echo \-n 'supersecret' | base64 \-\> c3VwZXJzZWNyZXQ=  
  API\_KEY: c3VwZXJzZWNyZXQ=

이제 세 가지 주입 방법을 보여주는 파드 예시입니다.

YAML

\# pod-injection-example.yaml  
apiVersion: v1  
kind: Pod  
metadata:  
  name: config-injection-demo  
spec:  
  containers:  
    \- name: my-app  
      image: busybox:1.36  
      \# 방법 1: 환경 변수로 주입  
      env:  
        \# ConfigMap에서 단일 값 주입  
        \- name: APP\_COLOR\_ENV  
          valueFrom:  
            configMapKeyRef:  
              name: app-config  
              key: APP\_COLOR  
        \# Secret에서 단일 값 주입  
        \- name: API\_KEY\_ENV  
          valueFrom:  
            secretKeyRef:  
              name: app-secret  
              key: API\_KEY  
      \# 방법 2: 커맨드 라인 인수로 주입  
      command:  
      \# 방법 3: 볼륨으로 마운트하여 파일로 주입  
      volumeMounts:  
        \- name: config-volume  
          mountPath: /etc/config  
  volumes:  
    \- name: config-volume  
      configMap:  
        name: app-config \# app-config의 모든 키-값 쌍이 파일로 마운트됨

#### **4.2 영구 스토리지: PV, PVC, StorageClass**

쿠버네티스에서 스토리지 관리는 컴퓨팅 관리와는 별개의 문제입니다.51 쿠버네티스는 세 가지 핵심 오브젝트를 통해 스토리지를 추상화합니다.

* **영구 볼륨 (PersistentVolume, PV)**: 클러스터 관리자가 프로비저닝했거나 동적으로 프로비저닝된 클러스터 내의 스토리지 조각입니다.52 PV는 특정 파드와 독립적인 생명주기를 가진 클러스터 리소스입니다.53  
* **영구 볼륨 클레임 (PersistentVolumeClaim, PVC)**: 사용자의 스토리지 요청입니다.52 파드가 노드의 리소스를 소비하는 것처럼, PVC는 PV 리소스를 소비합니다.  
* **스토리지 클래스 (StorageClass)**: 관리자가 제공하는 스토리지의 "등급"(예: fast-ssd, slow-hdd)을 정의하는 방법을 제공합니다.53

스토리지의 생명주기는 **프로비저닝 → 바인딩 → 사용 → 회수**의 단계를 거칩니다.51 핵심적인 차이점은

**정적 프로비저닝**(관리자가 PV를 미리 생성)과 **동적 프로비저닝**(PVC가 요청 시 StorageClass가 자동으로 PV를 생성)에 있으며, 동적 프로비저닝이 권장되는 방식입니다.51

**동적 프로비저닝 YAML 예시**

다음은 StorageClass, PVC, 그리고 이를 사용하는 파드를 포함한 완전한 동적 프로비저닝 예시입니다.

YAML

\# dynamic-provisioning.yaml  
\# 1\. StorageClass 정의  
apiVersion: storage.k8s.io/v1  
kind: StorageClass  
metadata:  
  name: standard-ssd  
provisioner: kubernetes.io/gce-pd \# 클라우드 제공업체에 따라 다름 (예: gce-pd, aws-ebs, azure-disk)  
parameters:  
  type: pd-ssd \# 빠른 SSD 타입 스토리지  
reclaimPolicy: Delete \# PVC가 삭제되면 PV도 함께 삭제

\---  
\# 2\. PersistentVolumeClaim 정의  
apiVersion: v1  
kind: PersistentVolumeClaim  
metadata:  
  name: my-app-pvc  
spec:  
  accessModes:  
    \- ReadWriteOnce \# 단일 노드에서만 읽기/쓰기 가능  
  storageClassName: standard-ssd \# 위에서 정의한 StorageClass 사용  
  resources:  
    requests:  
      storage: 5Gi \# 5GiB 스토리지 요청

\---  
\# 3\. PVC를 사용하는 Pod 정의  
apiVersion: v1  
kind: Pod  
metadata:  
  name: my-app-pod  
spec:  
  containers:  
    \- name: web-server  
      image: nginx:1.25  
      volumeMounts:  
        \- name: web-storage  
          mountPath: /usr/share/nginx/html  
  volumes:  
    \- name: web-storage  
      persistentVolumeClaim:  
        claimName: my-app-pvc \# 위에서 정의한 PVC 참조

#### **4.3 RBAC을 통한 접근 보안**

\*\*역할 기반 접근 제어(Role-Based Access Control, RBAC)\*\*는 클러스터 사용자 및 워크로드가 자신의 역할을 수행하는 데 필요한 최소한의 권한만 갖도록 보장하는 핵심 보안 제어입니다.54 RBAC은 네 가지 주요 오브젝트를 사용합니다.

* **Role**: 특정 네임스페이스 내의 리소스에 대한 권한 집합입니다.  
* **ClusterRole**: 클러스터 전체 범위의 권한 집합입니다 (네임스페이스에 국한되지 않음).  
* **RoleBinding**: 특정 네임스페이스 내에서 Role에 정의된 권한을 사용자, 그룹 또는 서비스 어카운트에 부여합니다.  
* **ClusterRoleBinding**: 클러스터 전체에 걸쳐 ClusterRole의 권한을 부여합니다.

이 네 가지 오브젝트는 "누가(Subject)" "무엇을(Permissions)" "어디서(Scope)" 할 수 있는지를 정의합니다. **최소 권한의 원칙**에 따라, 가능한 한 클러스터 전체 범위의 ClusterRole/ClusterRoleBinding 대신 네임스페이스 범위의 Role/RoleBinding을 사용하는 것이 중요합니다.54

**RBAC YAML 예시**

다음은 development 네임스페이스의 파드에 대한 읽기 전용 접근 권한을 정의하는 Role과, 이 역할을 dev-user라는 사용자에게 할당하는 RoleBinding의 예시입니다.

YAML

\# rbac-example.yaml  
\# 1\. Role 정의: 'development' 네임스페이스에서 파드에 대한 읽기 권한  
apiVersion: rbac.authorization.k8s.io/v1  
kind: Role  
metadata:  
  namespace: development  
  name: pod-reader  
rules:  
\- apiGroups: \[""\] \# ""는 코어 API 그룹을 의미  
  resources: \["pods"\]  
  verbs: \["get", "watch", "list"\]

\---  
\# 2\. RoleBinding 정의: 'pod-reader' 역할을 'dev-user'에게 부여  
apiVersion: rbac.authorization.k8s.io/v1  
kind: RoleBinding  
metadata:  
  name: read-pods  
  namespace: development  
subjects:  
\- kind: User  
  name: dev-user \# 권한을 부여할 사용자 이름  
  apiGroup: rbac.authorization.k8s.io  
roleRef:  
  kind: Role \# RoleBinding은 Role을 참조  
  name: pod-reader \# 위에서 정의한 Role의 이름  
  apiGroup: rbac.authorization.k8s.io

#### **4.4 파드 보안 표준(PSS)으로 워크로드 강화**

\*\*파드 보안 표준(Pod Security Standards, PSS)\*\*은 파드에 대한 세 가지 보안 정책을 정의하는 프레임워크입니다: Privileged(제한 없음), Baseline(알려진 권한 상승 방지), Restricted(강화된 모범 사례 따름).56 이는 복잡하고 폐기된 \*\*파드 보안 정책(PodSecurityPolicy, PSP)\*\*을 대체합니다.56

각 프로파일은 Restricted 프로파일이 non-root 사용자로 실행을 강제하고 대부분의 리눅스 커널 기능을 비활성화하는 등 특정 제한 사항을 정의합니다.58

**파드 보안 어드미션(Pod Security Admission, PSA)** 컨트롤러는 네임스페이스 수준에서 레이블을 사용하여 이러한 표준을 세 가지 모드로 적용합니다: enforce(위반 시 거부), audit(위반 사항 로깅), warn(사용자에게 경고).57

**PSS YAML 예시**

다음은 네임스페이스에 PSS를 적용하는 예시입니다. baseline 프로파일을 강제(enforce)하면서, 더 엄격한 restricted 프로파일 위반에 대해서는 감사(audit) 및 경고(warn)를 발생시켜 점진적인 보안 강화를 가능하게 합니다.

YAML

\# namespace-with-pss.yaml  
apiVersion: v1  
kind: Namespace  
metadata:  
  name: my-secure-namespace  
  labels:  
    \# Baseline 프로파일을 강제 모드로 설정  
    pod-security.kubernetes.io/enforce: baseline  
    \# Restricted 프로파일 위반 시 감사 로그 기록  
    pod-security.kubernetes.io/audit: restricted  
    \# Restricted 프로파일 위반 시 사용자에게 경고  
    pod-security.kubernetes.io/warn: restricted

### **섹션 5: 클러스터 내부 네트워킹**

이 섹션에서는 쿠버네티스 네트워킹의 복잡성을 해소하고, 서비스 간 통신 및 외부 트래픽이 애플리케이션에 도달하는 방법을 설명합니다.

#### **5.1 서비스(Service): 안정적인 엔드포인트 제공**

파드는 일시적이며 IP 주소가 변경될 수 있습니다.61 \*\*서비스(Service)\*\*는 논리적인 파드 집합과 이들에 접근하기 위한 정책을 정의하는 추상화로, 안정적인 IP 주소와 DNS 이름을 제공합니다.61

서비스는 레이블을 기반으로 한 selector를 사용하여 멤버 파드를 식별합니다.61 서비스에는 네 가지 주요 유형이 있습니다.

* **ClusterIP**: (기본값) 클러스터 내부 IP 주소에 서비스를 노출합니다. 클러스터 내에서만 접근 가능합니다.61  
* **NodePort**: 각 노드의 IP 주소에 정적 포트로 서비스를 노출합니다. 클러스터 외부에서 \<NodeIP\>:\<NodePort\>로 접근할 수 있습니다. ClusterIP 서비스가 자동으로 생성되어 NodePort 서비스가 라우팅됩니다.61  
* **LoadBalancer**: 클라우드 제공업체의 외부 로드 밸런서를 사용하여 서비스를 외부에 노출합니다. NodePort와 ClusterIP 서비스가 자동으로 생성되어 외부 로드 밸런서가 이쪽으로 트래픽을 라우팅합니다.61  
* **ExternalName**: 서비스를 외부 DNS 이름에 매핑하여 CNAME 레코드를 반환합니다. 프록시 기능은 없습니다.61

**서비스 YAML 예시**

다음은 app: my-app 레이블을 가진 파드들을 노출하는 NodePort 타입 서비스의 예시입니다.

YAML

\# service-nodeport.yaml  
apiVersion: v1  
kind: Service  
metadata:  
  name: my-app-service  
spec:  
  type: NodePort \# 서비스 타입  
  selector:  
    app: my-app \# 이 레이블을 가진 파드를 대상으로 함  
  ports:  
    \- protocol: TCP  
      port: 80 \# 클러스터 내부에서 서비스가 노출될 포트  
      targetPort: 8080 \# 파드가 리스닝하는 실제 포트  
      \# nodePort: 30007 \# 특정 포트를 지정할 수 있으나, 생략하면 자동 할당됨

#### **5.2 인그레스(Ingress): 외부 접근 관리**

\*\*인그레스(Ingress)\*\*는 클러스터 외부에서 클러스터 내부의 서비스로의 HTTP 및 HTTPS 접근을 관리하는 API 오브젝트입니다.65 인그레스는 로드 밸런싱, SSL/TLS 종료, 이름 기반 가상 호스팅과 같은 L7 라우팅 기능을 제공합니다.65 인그레스 리소스가 작동하려면

ingress-nginx와 같은 \*\*인그레스 컨트롤러(Ingress Controller)\*\*가 클러스터에 반드시 설치되어 있어야 합니다.65

인그레스는 호스트 이름과 경로 기반 규칙을 사용하여 트래픽을 여러 백엔드 서비스로 라우팅함으로써, 여러 개의 LoadBalancer 타입 서비스를 사용하는 것보다 훨씬 강력하고 유연한 외부 트래픽 관리 방법을 제공합니다.

**인그레스 YAML 예시**

다음은 호스트 이름과 경로에 따라 트래픽을 두 개의 다른 서비스로 라우팅하는 인그레스 예시입니다.

YAML

\# ingress-example.yaml  
apiVersion: networking.k8s.io/v1  
kind: Ingress  
metadata:  
  name: my-app-ingress  
  annotations:  
    nginx.ingress.kubernetes.io/rewrite-target: /  
spec:  
  rules:  
    \- host: "foo.bar.com"  
      http:  
        paths:  
          \- path: /service1  
            pathType: Prefix  
            backend:  
              service:  
                name: service1-name  
                port:  
                  number: 80  
    \- host: "bar.foo.com"  
      http:  
        paths:  
          \- path: /service2  
            pathType: Prefix  
            backend:  
              service:  
                name: service2-name  
                port:  
                  number: 80

#### **5.3 네트워크 정책(NetworkPolicy): 파드 간 통신 보안**

기본적으로 쿠버네티스 클러스터 내의 모든 파드는 제한 없이 서로 통신할 수 있습니다.67 \*\*네트워크 정책(NetworkPolicy)\*\*은 파드 그룹이 서로 또는 다른 네트워크 엔드포인트와 통신하는 방법을 지정할 수 있게 해줍니다.68 네트워크 정책을 사용하려면 Calico나 Cilium과 같이 이를 지원하는 CNI(Container Network Interface) 플러그인이 필요합니다.70

핵심 개념은 podSelector(정책이 적용될 파드), policyTypes(Ingress 또는 Egress), 그리고 ingress(어떤 소스로부터의 트래픽을 허용할지) 및 egress(어떤 목적지로의 트래픽을 허용할지) 규칙입니다. 보안 모범 사례는 **"기본 거부(default deny)"** 정책으로 시작하여 모든 파드를 격리한 다음, 필요한 트래픽 패턴만 명시적으로 허용하는 것입니다.71

**네트워크 정책 YAML 예시**

다음은 default 네임스페이스에 대한 "기본 거부" 정책과, frontend 파드에서 backend 파드로의 특정 인그레스 트래픽만 허용하는 정책의 예시입니다.

YAML

\# network-policy-example.yaml  
\# 1\. Default Deny All Ingress and Egress Traffic  
apiVersion: networking.k8s.io/v1  
kind: NetworkPolicy  
metadata:  
  name: default-deny-all  
  namespace: default  
spec:  
  podSelector: {} \# 네임스페이스의 모든 파드를 선택  
  policyTypes:  
    \- Ingress  
    \- Egress  
  \# ingress 및 egress 규칙이 비어 있으므로 모든 트래픽을 거부

\---  
\# 2\. Allow specific traffic from frontend to backend  
apiVersion: networking.k8s.io/v1  
kind: NetworkPolicy  
metadata:  
  name: backend-policy  
  namespace: default  
spec:  
  podSelector:  
    matchLabels:  
      app: backend \# 'app: backend' 레이블을 가진 파드에 이 정책 적용  
  policyTypes:  
    \- Ingress  
  ingress:  
    \- from:  
        \- podSelector:  
            matchLabels:  
              app: frontend \# 'app: frontend' 레이블을 가진 파드로부터의 트래픽만 허용  
      ports:  
        \- protocol: TCP  
          port: 8080 \# 8080 포트로의 TCP 트래픽만 허용

| 서비스 타입 | 주요 사용 사례 | 접근성 | 비용 영향 |
| :---- | :---- | :---- | :---- |
| **ClusterIP** | 클러스터 내부 통신 | 클러스터 내부에서만 접근 가능 | 없음 |
| **NodePort** | 개발/테스트용 외부 접근 | 노드 IP \+ 정적 포트 | 없음 |
| **LoadBalancer** | 프로덕션 환경의 외부 접근 | 외부 로드 밸런서 IP | 클라우드 제공업체의 로드 밸런서 비용 |
| **ExternalName** | 외부 서비스 참조 | DNS CNAME 리디렉션 | 없음 |

---

## **파트 3: 실전 가이드: 로컬 개발부터 프로덕션까지**

이 파트는 사용자가 쿠버네티스에서 전체 애플리케이션 생명주기를 직접 경험할 수 있도록 구성된 실습 튜토리얼입니다.

### **섹션 6: 단계별 튜토리얼: 웹 애플리케이션 배포**

#### **6.1 로컬 쿠버네티스 환경 설정**

로컬 머신에서 쿠버네티스를 실행하기 위한 두 가지 인기 있는 도구를 소개합니다.

* **Minikube**: 로컬 쿠버네티스를 위한 원조 도구입니다. VM 또는 Docker 컨테이너 내에서 단일 노드(또는 실험적인 다중 노드) 클러스터를 실행할 수 있습니다.75 풍부한 애드온을 제공하여 초보자에게 적합하지만 78, 리소스 사용량이 많을 수 있습니다.78  
  * **설치 및 시작**:  
    Bash  
    \# Minikube 설치 (운영체제에 따라 다름)  
    \#...  
    minikube start \--driver=docker \# Docker 드라이버로 클러스터 시작  
    minikube status \# 클러스터 상태 확인  
    minikube dashboard \# 웹 기반 대시보드 실행

* **kind (Kubernetes in Docker)**: Docker 컨테이너를 노드로 사용하여 쿠버네티스 클러스터를 실행하는 최신 도구입니다.80 매우 빠르고 가벼우며, 다중 노드 테스트 및 CI/CD 파이프라인에 탁월합니다.78  
  * **설치 및 시작**:  
    Bash  
    \# kind 설치 (운영체제에 따라 다름)  
    \#...  
    kind create cluster \--name my-cluster \# 단일 노드 클러스터 생성  
    kind delete cluster \--name my-cluster \# 클러스터 삭제

  * **다중 노드 클러스터 생성 예시 (kind-config.yaml)**:  
    YAML  
    kind: Cluster  
    apiVersion: kind.x-k8s.io/v1alpha4  
    nodes:  
    \- role: control-plane  
    \- role: worker  
    \- role: worker

    Bash  
    kind create cluster \--config kind-config.yaml

| 기능 | Minikube | kind |
| :---- | :---- | :---- |
| **기반 기술** | 가상 머신(VM) 또는 Docker | Docker-in-Docker |
| **리소스 사용량** | 높음 | 낮음 |
| **시작 시간** | 느림 | 빠름 |
| **다중 노드 지원** | 실험적 | 핵심 기능, 안정적 |
| **애드온 생태계** | 풍부하고 내장됨 | 수동 설정 필요 |
| **주요 사용 사례** | 기능이 풍부한 개발/학습 환경 | K8s 테스트, CI, 다중 노드 개발 |

#### **6.2 애플리케이션 빌드: 간단한 Python Flask 앱**

간단한 "Hello, World" 웹 애플리케이션을 위한 전체 소스 코드를 제공합니다.

* **app.py**:  
  Python  
  from flask import Flask  
  import os

  app \= Flask(\_\_name\_\_)

  @app.route('/')  
  def hello():  
      greeting \= os.environ.get("GREETING", "Hello")  
      return f"\<h1\>{greeting}, Kubernetes\!\</h1\>"

  if \_\_name\_\_ \== "\_\_main\_\_":  
      app.run(host='0.0.0.0', port=5000)

* **requirements.txt**:  
  Flask==3.0.3

* **Dockerfile**:  
  Dockerfile  
  \# 1\. 기본 이미지로 Python 3.9 slim 버전을 사용합니다.  
  FROM python:3.9\-slim

  \# 2\. 컨테이너 내부의 작업 디렉토리를 /app으로 설정합니다.  
  WORKDIR /app

  \# 3\. requirements.txt 파일을 컨테이너로 복사합니다.  
  COPY requirements.txt.

  \# 4\. pip를 사용하여 Python 종속성을 설치합니다.  
  RUN pip install \--no-cache-dir \-r requirements.txt

  \# 5\. 나머지 애플리케이션 소스 코드를 컨테이너로 복사합니다.  
  COPY..

  \# 6\. 컨테이너가 5000번 포트를 노출하도록 설정합니다.  
  EXPOSE 5000

  \# 7\. 컨테이너가 시작될 때 실행할 명령어를 정의합니다.  
  CMD \["python", "app.py"\]

#### **6.3 컨테이너화 및 레지스트리 푸시**

다음은 위에서 작성한 애플리케이션을 Docker 이미지로 빌드하고 Docker Hub에 푸시하는 단계별 명령어입니다.84

1. **Docker Hub 로그인**:  
   Bash  
   docker login  
   \# Docker Hub 사용자 이름과 비밀번호를 입력합니다.

2. **Docker 이미지 빌드**:  
   Bash  
   \# docker build \-t \<사용자이름\>/\<이미지이름\>:\<태그\>.  
   docker build \-t your-dockerhub-username/hello-k8s:v1.

3. **Docker 이미지 푸시**:  
   Bash  
   docker push your-dockerhub-username/hello-k8s:v1

#### **6.4 애플리케이션 배포 및 노출**

이제 빌드한 이미지를 사용하여 로컬 쿠버네티스 클러스터에 애플리케이션을 배포하고 노출합니다.

* **deployment.yaml**:  
  YAML  
  apiVersion: apps/v1  
  kind: Deployment  
  metadata:  
    name: hello-k8s-deployment  
  spec:  
    replicas: 2  
    selector:  
      matchLabels:  
        app: hello-k8s  
    template:  
      metadata:  
        labels:  
          app: hello-k8s  
      spec:  
        containers:  
        \- name: hello-k8s-container  
          image: your-dockerhub-username/hello-k8s:v1 \# 방금 푸시한 이미지  
          ports:  
          \- containerPort: 5000

* **service.yaml**:  
  YAML  
  apiVersion: v1  
  kind: Service  
  metadata:  
    name: hello-k8s-service  
  spec:  
    type: NodePort \# 로컬에서 쉽게 접근하기 위해 NodePort 타입 사용  
    selector:  
      app: hello-k8s  
    ports:  
    \- protocol: TCP  
      port: 80 \# 클러스터 내부에서 접근할 포트  
      targetPort: 5000 \# 컨테이너가 리스닝하는 포트

* **배포 및 확인**:  
  Bash  
  kubectl apply \-f deployment.yaml  
  kubectl apply \-f service.yaml

  \# 배포 상태 확인  
  kubectl get deployments  
  kubectl get pods

  \# 서비스 및 NodePort 확인  
  kubectl get services

* **애플리케이션 접근**:  
  Bash  
  \# Minikube의 경우  
  minikube service hello-k8s-service

  \# kind의 경우 (NodePort를 직접 포트 포워딩)  
  \# 먼저 NodePort 확인  
  NODE\_PORT=$(kubectl get svc hello-k8s-service \-o=jsonpath='{.spec.ports.nodePort}')  
  \# Docker 컨테이너의 포트를 로컬로 포워딩  
  docker port kind-control-plane $NODE\_PORT  
  \# 예: localhost:32123

#### **6.5 애플리케이션 생명주기 관리**

배포된 애플리케이션을 kubectl을 사용하여 관리하는 방법을 시연합니다.

* **스케일링**:  
  Bash  
  \# 복제본 수를 4개로 확장  
  kubectl scale deployment hello-k8s-deployment \--replicas=4  
  kubectl get pods \# 4개의 파드가 실행 중인지 확인

* **롤링 업데이트**:  
  1. app.py를 수정하고, Dockerfile을 변경하지 않은 채 새로운 버전의 이미지를 빌드하고 푸시합니다 (예: your-dockerhub-username/hello-k8s:v2).  
  2. kubectl set image 명령어로 디플로이먼트의 컨테이너 이미지를 업데이트합니다.  
     Bash  
     kubectl set image deployment/hello-k8s-deployment hello-k8s-container=your-dockerhub-username/hello-k8s:v2

  3. 롤링 업데이트 진행 상황을 모니터링합니다.  
     Bash  
     kubectl rollout status deployment/hello-k8s-deployment

     이 과정에서 쿠버네티스는 새로운 파드를 점진적으로 생성하고 이전 파드를 종료하여 무중단 업데이트를 보장합니다.38  
* **롤백**:  
  1. 업데이트에 문제가 발생했다고 가정합니다. 먼저, 배포 기록을 확인합니다.  
     Bash  
     kubectl rollout history deployment/hello-k8s-deployment

  2. kubectl rollout undo 명령어로 이전 버전으로 롤백합니다.  
     Bash  
     kubectl rollout undo deployment/hello-k8s-deployment

     이 명령은 디플로이먼트를 이전의 안정적인 리비전으로 되돌립니다.38

### **섹션 7: 모니터링 및 로깅**

이 섹션에서는 쿠버네티스 환경에서의 가시성 확보에 필수적인 모니터링과 로깅을 다룹니다.

#### **7.1 프로메테우스와 그라파나를 이용한 모니터링**

\*\*프로메테우스(Prometheus)\*\*는 쿠버네티스 모니터링의 사실상 표준으로, 시계열 메트릭을 수집합니다.86 \*\*그라파나(Grafana)\*\*는 이 메트릭을 대시보드 형태로 시각화하는 데 사용됩니다.87 이들을 가장 쉽게 설치하는 방법은

kube-prometheus-stack 헬름 차트를 사용하는 것입니다.86

이 아키텍처의 핵심은 \*\*프로메테우스 오퍼레이터(Prometheus Operator)\*\*입니다. 오퍼레이터는 ServiceMonitor와 PodMonitor라는 CRD(Custom Resource Definition)를 사용하여 스크레이프 대상을 자동으로 발견하고 구성하는 작업을 단순화합니다.90

**튜토리얼: kube-prometheus-stack 설치 및 사용**

1. **헬름 리포지토리 추가 및 업데이트**:  
   Bash  
   helm repo add prometheus-community https://prometheus-community.github.io/helm-charts  
   helm repo update

2. **kube-prometheus-stack 헬름 차트 설치**:  
   Bash  
   helm install my-prometheus prometheus-community/kube-prometheus-stack \--namespace monitoring \--create-namespace

3. 프로메테우스 및 그라파나 UI 접근:  
   port-forward를 사용하여 로컬에서 UI에 접근합니다.  
   Bash  
   \# 프로메테우스 UI (http://localhost:9090)  
   kubectl port-forward \-n monitoring svc/my-prometheus-kube-prometheus-prometheus 9090:9090

   \# 그라파나 UI (http://localhost:3000)  
   kubectl port-forward \-n monitoring svc/my-prometheus-grafana 3000:3000

   그라파나 초기 비밀번호는 다음 명령으로 확인할 수 있습니다:  
   Bash  
   kubectl get secret \-n monitoring my-prometheus-grafana \-o jsonpath="{.data.admin-password}" | base64 \--decode ; echo

4. 사용자 정의 애플리케이션 모니터링:  
   위에서 배포한 hello-k8s 애플리케이션의 메트릭을 수집하기 위해 ServiceMonitor를 생성합니다. (이를 위해 애플리케이션은 /metrics 엔드포인트에서 프로메테우스 형식의 메트릭을 노출해야 합니다.)  
   YAML  
   \# servicemonitor-hello-k8s.yaml  
   apiVersion: monitoring.coreos.com/v1  
   kind: ServiceMonitor  
   metadata:  
     name: hello-k8s-sm  
     namespace: monitoring \# 프로메테우스가 설치된 네임스페이스  
     labels:  
       release: my-prometheus \# 프로메테우스가 이 ServiceMonitor를 선택하도록 레이블 설정  
   spec:  
     selector:  
       matchLabels:  
         app: hello-k8s \# 모니터링할 서비스의 레이블  
     namespaceSelector:  
       matchNames:  
       \- default \# 서비스가 있는 네임스페이스  
     endpoints:  
     \- port: http \# 서비스의 포트 이름  
       interval: 15s

   Bash  
   kubectl apply \-f servicemonitor-hello-k8s.yaml

   이제 프로메테우스 UI의 'Targets' 페이지에서 hello-k8s 엔드포인트가 스크레이프 대상으로 추가된 것을 확인할 수 있습니다.

#### **7.2 EFK 스택을 이용한 중앙 집중식 로깅**

쿠버네티스와 같은 분산 시스템에서는 디버깅과 모니터링을 위해 중앙 집중식 로깅이 필수적입니다.94 \*\*EFK 스택 (Elasticsearch, Fluentd, Kibana)\*\*은 널리 사용되는 오픈소스 로깅 솔루션입니다.94

**아키텍처 및 데이터 흐름** 94:

1. **Fluentd**: **데몬셋**으로 배포되어 모든 노드의 컨테이너 로그를 수집합니다.97 로그에 파드 이름, 네임스페이스와 같은 쿠버네티스 메타데이터를 추가하여 강화한 후 Elasticsearch로 전달합니다.98  
2. **Elasticsearch**: 로그를 저장하고 인덱싱하는 분산 검색 및 분석 엔진입니다.95 데이터 영속성을 위해  
   **스테이트풀셋**으로 배포해야 합니다.100  
3. **Kibana**: Elasticsearch에 저장된 로그 데이터를 검색, 분석, 시각화하는 웹 UI입니다.95

**튜토리얼: EFK 스택 배포**

다음은 EFK 스택의 각 컴포넌트를 배포하기 위한 간략한 YAML 예시입니다.

* **Elasticsearch (StatefulSet)**:  
  YAML  
  \# elasticsearch-sts.yaml  
  apiVersion: apps/v1  
  kind: StatefulSet  
  metadata:  
    name: elasticsearch  
  \#... (안정적인 스토리지와 네트워크를 위한 상세 설정)...  
  spec:  
    serviceName: "elasticsearch"  
    replicas: 3  
    template:  
      spec:  
        containers:  
        \- name: elasticsearch  
          image: docker.elastic.co/elasticsearch/elasticsearch:8.13.2  
          \#... (리소스, 포트, 환경 변수 설정)...  
    volumeClaimTemplates:  
    \- metadata:  
        name: data  
      spec:  
        accessModes:  
        storageClassName: "standard"  
        resources:  
          requests:  
            storage: 10Gi

* **Fluentd (DaemonSet & ConfigMap)**:  
  YAML  
  \# fluentd-daemonset.yaml  
  apiVersion: apps/v1  
  kind: DaemonSet  
  metadata:  
    name: fluentd  
  spec:  
    template:  
      spec:  
        containers:  
        \- name: fluentd  
          image: fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch  
          env:  
            \- name: FLUENT\_ELASTICSEARCH\_HOST  
              value: "elasticsearch-logging.default.svc.cluster.local"  
          volumeMounts:  
          \- name: varlog  
            mountPath: /var/log  
          \#... (기타 볼륨 마운트)...  
        volumes:  
        \- name: varlog  
          hostPath:  
            path: /var/log

* **Kibana (Deployment & Service)**:  
  YAML  
  \# kibana-deployment.yaml  
  apiVersion: apps/v1  
  kind: Deployment  
  metadata:  
    name: kibana  
  spec:  
    replicas: 1  
    template:  
      spec:  
        containers:  
        \- name: kibana  
          image: docker.elastic.co/kibana/kibana:8.13.2  
          env:  
            \- name: ELASTICSEARCH\_HOSTS  
              value: "http://elasticsearch:9200"  
          ports:  
          \- containerPort: 5601  
  \---  
  apiVersion: v1  
  kind: Service  
  metadata:  
    name: kibana  
  spec:  
    type: NodePort  
    ports:  
    \- port: 5601  
      targetPort: 5601  
    selector:  
      app: kibana

### **섹션 8: 자동화 및 CI/CD**

이 섹션에서는 애플리케이션 생명주기를 자동화하는 방법을 다룹니다.

#### **8.1 헬름(Helm)을 이용한 패키지 관리**

\*\*헬름(Helm)\*\*은 쿠버네티스를 위한 패키지 관리자입니다.101 헬름은 \*\*차트(Charts)\*\*라는 패키징 형식을 사용하여 애플리케이션을 정의, 설치, 업그레이드합니다.103 \*\*릴리스(Release)\*\*는 클러스터에서 실행 중인 차트의 인스턴스입니다.102

헬름은 복잡성을 관리하고, 반복 가능한 설치를 제공하며, YAML 파일을 "복사-붙여넣기"하는 방식에서 벗어나게 해주므로 매우 중요합니다.101 헬름 차트의 구조는 다음과 같습니다 105:

* Chart.yaml: 차트 이름, 버전과 같은 메타데이터를 포함합니다.  
* values.yaml: 차트의 기본 구성 값을 정의합니다. 이 값들은 설치 시 재정의될 수 있습니다.  
* templates/ 디렉토리: 쿠버네티스 매니페스트 템플릿 파일들을 포함합니다.

**튜토리얼: 헬름 기본 워크플로우**

1. **차트 생성**:  
   Bash  
   helm create my-app

2. **애플리케이션 설치**:  
   Bash  
   helm install my-release./my-app

3. **애플리케이션 업그레이드**: values.yaml 파일을 수정하거나 \--set 플래그를 사용하여 구성 변경 후 업그레이드합니다.  
   Bash  
   helm upgrade my-release./my-app \--set image.tag=v2

4. **애플리케이션 롤백**:  
   Bash  
   helm rollback my-release 1 \# 리비전 1로 롤백

#### **8.2 Jenkins와 ArgoCD를 이용한 CI/CD 파이프라인 구현 (GitOps)**

* **Jenkins**: CI(Continuous Integration)를 위해 널리 사용되는 오픈소스 자동화 서버로, 애플리케이션을 빌드, 테스트, 패키징하는 데 사용됩니다.110  
* **ArgoCD**: 쿠버네티스를 위한 선언적 GitOps 기반의 CD(Continuous Delivery) 도구입니다.55  
* **GitOps**: Git 리포지토리를 선언적 인프라와 애플리케이션의 단일 진실 공급원(single source of truth)으로 사용하는 방식입니다.55

현대적인 CI/CD 파이프라인은 다음과 같이 작동합니다:

1. 개발자가 애플리케이션 코드 Git 리포지토리에 코드를 푸시합니다.  
2. 웹훅이 \*\*Jenkins 파이프라인(CI)\*\*을 트리거합니다. Jenkins는 코드를 체크아웃하고, 테스트를 실행하며, 새로운 Docker 이미지를 빌드하여 레지스트리에 푸시합니다. 그런 다음, Jenkins는 **별도의 GitOps 리포지토리**에 있는 쿠버네티스 매니페스트의 이미지 태그를 업데이트합니다.  
3. 클러스터에서 실행 중인 \*\*ArgoCD(CD)\*\*는 GitOps 리포지토리의 변경 사항을 감지합니다. ArgoCD는 업데이트된 매니페스트를 자동으로 가져와 클러스터에 적용하여, 클러스터의 상태를 Git에 정의된 상태와 일치시킵니다.

이러한 **풀(Pull) 기반 GitOps 모델**은 전통적인 푸시(Push) 기반 모델보다 본질적으로 더 안전하고 신뢰할 수 있습니다. 전통적인 방식에서는 CI 서버(Jenkins)가 쿠버네티스 클러스터에 직접 접근할 수 있는 강력한 자격 증명을 가져야 하므로, CI 서버가 손상될 경우 큰 보안 위험이 됩니다.55 반면, 풀 기반 모델에서는 CD 에이전트(ArgoCD)가 클러스터

*내부*에 존재하며, Git 리포지토리에 대한 읽기 전용 자격 증명만 필요로 합니다.55 이는 클러스터 자격 증명이 외부 시스템에 노출되지 않도록 하여 공격 표면을 줄입니다. 또한, Git 리포지토리가 단일 진실 공급원이 되므로, 수동 변경으로 인해 클러스터 상태가 변경되더라도 ArgoCD의 자체 복구 기능이 Git에 정의된 상태로 자동 복원하여 구성 드리프트를 방지하고 모든 변경에 대한 명확한 감사 추적을 제공합니다.55

**코드 예시**

* **Jenkinsfile (Declarative Pipeline)**:  
  Groovy  
  pipeline {  
      agent any  
      environment {  
          DOCKER\_HUB\_CREDENTIALS \= credentials('dockerhub-creds')  
          GIT\_OPS\_REPO \= 'https://github.com/your-username/gitops-repo.git'  
      }  
      stages {  
          stage('Checkout') {  
              steps {  
                  git 'https://github.com/your-username/app-repo.git'  
              }  
          }  
          stage('Build Docker Image') {  
              steps {  
                  script {  
                      def image \= docker.build("your-dockerhub-username/hello-k8s:${env.BUILD\_ID}")  
                  }  
              }  
          }  
          stage('Push to Docker Hub') {  
              steps {  
                  script {  
                      docker.withRegistry('https://registry.hub.docker.com', DOCKER\_HUB\_CREDENTIALS) {  
                          docker.image("your-dockerhub-username/hello-k8s:${env.BUILD\_ID}").push()  
                      }  
                  }  
              }  
          }  
          stage('Update GitOps Repo') {  
              steps {  
                  // GitOps 리포지토리 클론, YAML 파일 수정, 커밋 및 푸시  
                  sh """  
                  git clone ${GIT\_OPS\_REPO}  
                  cd gitops-repo  
                  \# kustomize, sed, yq 등 도구를 사용하여 deployment.yaml의 이미지 태그 업데이트  
                  git config user.email "jenkins@example.com"  
                  git config user.name "Jenkins"  
                  git add.  
                  git commit \-m "Update image to version ${env.BUILD\_ID}"  
                  git push  
                  """  
              }  
          }  
      }  
  }

* **ArgoCD Application CRD YAML**:  
  YAML  
  \# argocd-application.yaml  
  apiVersion: argoproj.io/v1alpha1  
  kind: Application  
  metadata:  
    name: hello-k8s-app  
    namespace: argocd  
  spec:  
    project: default  
    source:  
      repoURL: 'https://github.com/your-username/gitops-repo.git' \# GitOps 리포지토리  
      path: 'path/to/manifests' \# 매니페스트가 있는 경로  
      targetRevision: HEAD  
    destination:  
      server: 'https://kubernetes.default.svc' \# 대상 클러스터  
      namespace: 'production' \# 대상 네임스페이스  
    syncPolicy:  
      automated:  
        prune: true \# Git에서 사라진 리소스는 클러스터에서도 삭제  
        selfHeal: true \# 클러스터 상태가 Git과 다르면 자동 복구

---

## **파트 4: 더 넓은 생태계**

이 파트에서는 고급 주제와 관리형 서비스를 탐색하여 사용자에게 다음 단계로 나아갈 길을 제시합니다.

### **섹션 9: 고급 네트워킹 및 서버리스**

#### **9.1 서비스 메시 소개: Istio와 Linkerd**

\*\*서비스 메시(Service Mesh)\*\*는 서비스 간의 통신을 안전하고, 빠르며, 신뢰할 수 있게 만드는 전용 인프라 계층입니다.115 이는 각 서비스 인스턴스 옆에 네트워크 프록시(

**사이드카**)를 배포하여 구현됩니다.115 서비스 메시는 프록시들로 구성된 \*\*데이터 플레인(Data Plane)\*\*과 이 프록시들을 관리하는 \*\*컨트롤 플레인(Control Plane)\*\*으로 구성됩니다.116

가장 널리 사용되는 두 서비스 메시는 다음과 같습니다.

* **Linkerd**: 단순성, 고성능, 낮은 리소스 사용량에 중점을 둡니다. Rust로 작성된 경량의 목적별 프록시를 사용하며 109, "그냥 작동하는" 쉬운 운영과 기본적으로 활성화되는 mTLS(상호 TLS) 보안으로 유명합니다.109  
* **Istio**: 더 포괄적이고 강력한 기능 세트를 제공합니다. C++로 작성된 업계 표준 Envoy 프록시를 사용하며 117,  
  VirtualService 및 DestinationRule CRD를 통해 트래픽 분할, 결함 주입, 서킷 브레이커와 같은 고급 트래픽 관리 기능을 제공합니다.120 하지만 구성과 운영이 더 복잡한 것으로 알려져 있습니다.109

**Istio를 이용한 카나리 배포 YAML 예시**

다음은 VirtualService와 DestinationRule을 사용하여 90%의 트래픽을 v1으로, 10%의 트래픽을 v2(카나리)로 보내는 예시입니다.

YAML

\# istio-canary.yaml  
apiVersion: networking.istio.io/v1  
kind: DestinationRule  
metadata:  
  name: my-app-destination  
spec:  
  host: my-app-service  
  subsets:  
  \- name: v1  
    labels:  
      version: v1  
  \- name: v2  
    labels:  
      version: v2  
\---  
apiVersion: networking.istio.io/v1  
kind: VirtualService  
metadata:  
  name: my-app-virtualservice  
spec:  
  hosts:  
  \- my-app-service  
  http:  
  \- route:  
    \- destination:  
        host: my-app-service  
        subset: v1  
      weight: 90  
    \- destination:  
        host: my-app-service  
        subset: v2  
      weight: 10

#### **9.2 Knative를 이용한 쿠버네티스 서버리스**

**Knative**는 쿠버네티스 위에서 서버리스, 클라우드 네이티브 애플리케이션을 배포, 실행, 관리하기 위한 컴포넌트를 추가하는 오픈소스 프로젝트입니다.123

Knative의 핵심 컴포넌트는 다음과 같습니다:

* **Serving**: 요청 기반 컴퓨팅을 제공하며, 트래픽이 없을 때 파드 수를 0으로 줄이는 **Scale-to-Zero** 기능을 지원합니다.124  
* **Eventing**: 이벤트 소스로부터 이벤트를 소비하고 애플리케이션으로 전달하는 인프라를 제공합니다.124

Knative의 가장 큰 장점은 AWS Lambda나 Azure Functions와 같은 특정 벤더의 FaaS(Function-as-a-Service) 플랫폼에 종속되지 않고, 모든 쿠버네티스 클러스터에서 서버리스 워크로드를 실행할 수 있다는 점입니다.125 Knative는

Service, Route, Configuration, Revision이라는 핵심 리소스를 통해 점진적 롤아웃을 위한 트래픽 분할과 같은 기능을 제공합니다.126

**Knative Service YAML 예시**

다음은 Scale-to-Zero 기능과 새로운 리비전으로의 트래픽 분할을 보여주는 Knative Service 예시입니다.

YAML

\# knative-service.yaml  
apiVersion: serving.knative.dev/v1  
kind: Service  
metadata:  
  name: helloworld-go  
spec:  
  template:  
    spec:  
      containers:  
        \- image: gcr.io/knative-samples/helloworld-go  
          env:  
            \- name: TARGET  
              value: "Knative User"  
  \# 90% 트래픽은 안정적인 최신 리비전으로, 10%는 특정 리비전(카나리)으로 분할  
  traffic:  
  \- percent: 90  
    latestRevision: true  
  \- percent: 10  
    revisionName: helloworld-go-abcde \# 특정 리비전 이름  
    tag: canary

### **섹션 10: 관리형 쿠버네티스 서비스**

이 섹션에서는 3대 클라우드 제공업체의 관리형 쿠버네티스 서비스에 대한 개요와 상세 비교를 제공합니다.

#### **10.1 클라우드 관리형 쿠버네티스 개요**

관리형 쿠버네티스 서비스는 쿠버네티스 컨트롤 플레인(그리고 종종 노드까지)을 관리하는 운영 부담을 클라우드 제공업체에 위임하여, 팀이 애플리케이션에 더 집중할 수 있도록 해줍니다.127

#### **10.2 Amazon Elastic Kubernetes Service (EKS)**

EKS는 고가용성을 위해 여러 AWS 가용 영역(AZ)에 걸쳐 관리형 컨트롤 플레인을 제공합니다.130 IAM(RBAC용), VPC(네트워킹용), ELB(로드 밸런싱용) 등 AWS 생태계와 깊이 통합되어 있습니다.130 노드 관리는

**Managed Node Groups** 또는 서버리스 컴퓨팅인 **AWS Fargate**를 통해 처리됩니다.130

#### **10.3 Google Kubernetes Engine (GKE)**

쿠버네티스의 원조 개발사인 구글이 제공하는 GKE는 가장 성숙하고 기능이 풍부한 관리형 서비스로 평가받습니다.127 GKE는 두 가지 운영 모드를 제공합니다: 사용자가 노드를 관리하는

**Standard** 모드와, GKE가 노드와 컨트롤 플레인을 모두 관리하여 서버리스와 유사한 경험을 제공하는 **Autopilot** 모드입니다.134 뛰어난 오토스케일링, 자동 업그레이드, Google Cloud의 모니터링 및 로깅 도구와의 깊은 통합이 특징입니다.134

#### **10.4 Azure Kubernetes Service (AKS)**

AKS는 사용 편의성과 개발자 친화적인 경험으로 잘 알려져 있습니다.128 종종 무료로 제공되는 관리형 컨트롤 플레인을 갖추고 있으며, Azure Active Directory(인증용), Azure Monitor, Azure Policy 등 Azure 서비스와 원활하게 통합됩니다.128 또한, \*\*가상 노드(Virtual Nodes)\*\*를 통해 Azure Container Instances(ACI)로 버스팅하여 서버리스 컴퓨팅을 활용할 수 있습니다.128

#### **10.5 EKS, GKE, AKS 상세 비교 분석**

세 플랫폼 모두 CNCF 인증을 받았으며 핵심적인 관리형 쿠버네티스 기능을 제공합니다.129 이들의 차이점은 핵심 쿠버네티스 기능이 아닌, 각 플랫폼의 철학과 생태계 통합 방식에 있습니다.

* **생태계**: 선택은 기존 클라우드 투자에 크게 좌우됩니다. EKS는 IAM/VPC와의 깊은 통합으로 AWS 중심의 환경에 최적화되어 있습니다.130 AKS는 AAD 통합으로 Azure 중심 조직에 논리적인 선택입니다.129 GKE는 네트워킹, AI/ML, 가시성 등 강력한 Google Cloud 생태계를 활용합니다.127  
* **운영 모델**: 각 플랫폼은 다른 운영 철학을 지향합니다. GKE의 Autopilot은 인프라를 완전히 추상화하고자 하는 팀에게 이상적인 "제로 옵스(zero-ops)" 경험을 제공합니다.133 EKS는 데이터 플레인(예: VPC CNI를 통한 네트워킹)에 대한 세밀한 제어를 제공하여, AWS 인프라를 깊이 제어해야 하는 팀에게 매력적입니다.141 AKS는 이 둘 사이의 균형을 맞추며, 가장 시작하기 쉬운 서비스로 평가받기도 합니다.138  
* **가격 모델**: 가격 모델 또한 이러한 철학을 반영합니다. AKS와 GKE(Standard)는 무료 컨트롤 플레인을 제공하여 소규모 클러스터에 더 저렴합니다.140 EKS는 컨트롤 플레인에 대해 비용을 부과하며, 이는 전용 단일 테넌트 특성을 반영합니다.140 GKE Autopilot과 EKS on Fargate는 사용량 기반 가격 모델을 채택하여, 변동성이 큰 워크로드에는 저렴할 수 있지만 안정적인 워크로드에는 더 비쌀 수 있습니다.145  
* **하이브리드/멀티 클라우드**: GKE의 Anthos는 AWS와 Azure의 클러스터까지 관리할 수 있는 진정한 멀티 클라우드 컨트롤 플레인으로 포지셔닝되어 있습니다. EKS Anywhere와 AKS with Azure Arc는 각자의 클라우드를 온프레미스 환경으로 확장하는 데 더 중점을 둡니다.140

결론적으로, 단 하나의 "최고의" 제공업체는 없습니다. 최적의 선택은 조직의 기존 클라우드 제공업체, 원하는 운영 제어 수준("제로 옵스"부터 "세밀한 제어"까지), 그리고 특정 기술적 요구사항에 따라 달라집니다.

| 항목 | Amazon EKS | Google Kubernetes Engine (GKE) | Azure Kubernetes Service (AKS) |
| :---- | :---- | :---- | :---- |
| **컨트롤 플레인** | 단일 테넌트, 유료 ($0.10/시간) | Google 관리, 첫 Zonal 클러스터 무료 | Azure 관리, 무료 |
| **노드 관리** | Managed Node Groups, Fargate(서버리스) | Standard(수동), Autopilot(관리형) | VM Scale Sets, Virtual Nodes(ACI) |
| **오토스케일링** | Karpenter, Cluster Autoscaler | HPA, VPA, Cluster Autoscaler | Cluster Autoscaler, KEDA |
| **네트워킹 (CNI)** | AWS VPC CNI | GKE Dataplane V2 (Cilium 기반) | Azure CNI, Kubenet |
| **스토리지 (CSI)** | EBS, EFS CSI 드라이버 | Persistent Disk, Filestore CSI 드라이버 | Azure Disk, Azure Files CSI 드라이버 |
| **보안 (인증)** | AWS IAM 통합 | Google Cloud IAM | Azure Active Directory 통합 |
| **가시성 통합** | Amazon CloudWatch | Google Cloud's operations suite | Azure Monitor |
| **하이브리드/멀티 클라우드** | EKS Anywhere, EKS Distro | Anthos | Azure Arc |
| **가격 모델** | 클러스터당 요금 \+ 리소스 비용 | 클러스터당 요금(Standard) 또는 파드 리소스 요청당(Autopilot) \+ 리소스 비용 | 컨트롤 플레인 무료 \+ 리소스 비용 |
| **최적 환경** | AWS 생태계와의 깊은 통합 및 제어가 필요한 경우 | 성숙한 K8s 기능, '제로 옵스', 멀티 클라우드가 필요한 경우 | 사용 편의성, Azure 및 Windows 환경에 최적 |

---

### **부록: kubectl 명령어 참조**

#### **A.1 kubectl 치트 시트**

이 섹션은 가장 일반적으로 사용되는 kubectl 명령어에 대한 빠른 참조 가이드입니다.

| 분류 | 명령어 | 설명 | 주요 옵션/플래그 |
| :---- | :---- | :---- | :---- |
| **클러스터 및 컨텍스트** | kubectl cluster-info | 클러스터의 컨트롤 플레인 및 서비스 주소를 표시합니다. |  |
|  | kubectl config get-contexts | 정의된 모든 컨텍스트를 나열합니다. |  |
|  | kubectl config use-context \<name\> | 현재 컨텍스트를 지정된 컨텍스트로 전환합니다. |  |
| **리소스 조회** | kubectl get \<resource\> | 하나 이상의 리소스를 나열합니다. (예: pods, svc, deploy) | \-n \<ns\>, \-o wide, \-l \<label\>, \-A (모든 네임스페이스) |
|  | kubectl describe \<resource\> \<name\> | 특정 리소스의 상세 상태를 표시합니다. (이벤트 포함) | \-n \<ns\> |
|  | kubectl logs \<pod-name\> | 파드의 로그를 출력합니다. | \-f (실시간), \-c \<container\>, \--previous |
|  | kubectl top node/pod | 노드 또는 파드의 리소스(CPU/메모리) 사용량을 표시합니다. | \-n \<ns\> |
| **리소스 관리** | kubectl apply \-f \<file.yaml\> | 파일이나 디렉토리의 구성으로 리소스를 생성하거나 업데이트합니다. | \--dry-run=client |
|  | kubectl create \<resource\> \<name\> | 특정 리소스를 생성합니다. (예: namespace) |  |
|  | kubectl edit \<resource\> \<name\> | 서버에서 리소스의 정의를 편집합니다. |  |
|  | kubectl delete \<resource\> \<name\> | 특정 리소스를 삭제합니다. | \-f \<file.yaml\>, \-l \<label\> |
|  | kubectl scale deployment \<name\> | 디플로이먼트의 복제본 수를 조정합니다. | \--replicas=\<count\> |
| **배포 관리** | kubectl rollout status \<deployment\> | 디플로이먼트의 롤아웃 상태를 확인합니다. |  |
|  | kubectl rollout history \<deployment\> | 디플로이먼트의 롤아웃 기록을 확인합니다. | \--revision=\<num\> |
|  | kubectl rollout undo \<deployment\> | 이전 디플로이먼트로 롤백합니다. | \--to-revision=\<num\> |
| **디버깅** | kubectl exec \-it \<pod-name\> \-- \<cmd\> | 파드 내의 컨테이너에서 명령어를 실행합니다. (예: \-- /bin/sh) | \-c \<container\> |
|  | kubectl port-forward \<pod/svc\> \<local\>:\<remote\> | 로컬 포트를 파드 또는 서비스의 포트로 포워딩합니다. |  |
|  | kubectl debug \<pod-name\> | 실행 중인 파드에 대한 디버깅 세션을 시작합니다. |  |

#### **참고 자료**

1. What is Container Orchestration? \- VMware, 8월 27, 2025에 액세스, [https://www.vmware.com/topics/container-orchestration](https://www.vmware.com/topics/container-orchestration)  
2. What is container orchestration? | Google Cloud, 8월 27, 2025에 액세스, [https://cloud.google.com/discover/what-is-container-orchestration](https://cloud.google.com/discover/what-is-container-orchestration)  
3. cloud.google.com, 8월 27, 2025에 액세스, [https://cloud.google.com/discover/what-is-container-orchestration\#:\~:text=Container%20orchestration%20is%20the%20system,life%20cycle%20management%20of%20containers.](https://cloud.google.com/discover/what-is-container-orchestration#:~:text=Container%20orchestration%20is%20the%20system,life%20cycle%20management%20of%20containers.)  
4. What is Container Orchestration? \- AWS, 8월 27, 2025에 액세스, [https://aws.amazon.com/what-is/container-orchestration/](https://aws.amazon.com/what-is/container-orchestration/)  
5. What is container orchestration? \- Red Hat, 8월 27, 2025에 액세스, [https://www.redhat.com/en/topics/containers/what-is-container-orchestration](https://www.redhat.com/en/topics/containers/what-is-container-orchestration)  
6. Understanding Kubernetes Architecture: A Comprehensive Guide | by Salwan Mohamed, 8월 27, 2025에 액세스, [https://medium.com/@salwan.mohamed/understanding-kubernetes-architecture-a-comprehensive-guide-0f69317aa5c5](https://medium.com/@salwan.mohamed/understanding-kubernetes-architecture-a-comprehensive-guide-0f69317aa5c5)  
7. Kubernetes, 8월 27, 2025에 액세스, [https://kubernetes.io/](https://kubernetes.io/)  
8. Top 7 Key Benefits of Kubernetes to Consider in 2024 \- groundcover, 8월 27, 2025에 액세스, [https://www.groundcover.com/blog/kubernetes-benefits](https://www.groundcover.com/blog/kubernetes-benefits)  
9. The History of Kubernetes | IBM, 8월 27, 2025에 액세스, [https://www.ibm.com/think/topics/kubernetes-history](https://www.ibm.com/think/topics/kubernetes-history)  
10. The Evolution of Kubernetes: From Borg to K8s and How it Became the Standard for Container Orchestration \- Roman Glushach, 8월 27, 2025에 액세스, [https://romanglushach.medium.com/the-evolution-of-kubernetes-from-borg-to-k8s-and-how-it-became-the-standard-for-container-7700dcdf883b](https://romanglushach.medium.com/the-evolution-of-kubernetes-from-borg-to-k8s-and-how-it-became-the-standard-for-container-7700dcdf883b)  
11. Borg, Omega, and Kubernetes \- Google Research, 8월 27, 2025에 액세스, [https://research.google.com/pubs/archive/44843.pdf](https://research.google.com/pubs/archive/44843.pdf)  
12. Borg: The Predecessor to Kubernetes, 8월 27, 2025에 액세스, [https://kubernetes.io/blog/2015/04/borg-predecessor-to-kubernetes/](https://kubernetes.io/blog/2015/04/borg-predecessor-to-kubernetes/)  
13. Kubernetes \- Wikipedia, 8월 27, 2025에 액세스, [https://en.wikipedia.org/wiki/Kubernetes](https://en.wikipedia.org/wiki/Kubernetes)  
14. Is Kubernetes Worth It? A 2024 Guide to Cost & Benefits, 8월 27, 2025에 액세스, [https://www.plural.sh/blog/is-kubernetes-worth-it/](https://www.plural.sh/blog/is-kubernetes-worth-it/)  
15. Kubernetes \- Architecture \- GeeksforGeeks, 8월 27, 2025에 액세스, [https://www.geeksforgeeks.org/devops/kubernetes-architecture/](https://www.geeksforgeeks.org/devops/kubernetes-architecture/)  
16. Kubernetes Architecture: Control Plane, Data Plane, and 11 Core Components Explained, 8월 27, 2025에 액세스, [https://spot.io/resources/kubernetes-architecture/11-core-components-explained/](https://spot.io/resources/kubernetes-architecture/11-core-components-explained/)  
17. Kubernetes Architecture: The Definitive Guide (2025) \- DevOpsCube, 8월 27, 2025에 액세스, [https://devopscube.com/kubernetes-architecture-explained/](https://devopscube.com/kubernetes-architecture-explained/)  
18. Kubernetes Architecture \- KodeKloud Notes, 8월 27, 2025에 액세스, [https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Kubernetes/Kubernetes-Architecture](https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Kubernetes/Kubernetes-Architecture)  
19. Kubernetes Control Plane: Ultimate Guide (2024) \- Plural.sh, 8월 27, 2025에 액세스, [https://www.plural.sh/blog/kubernetes-control-plane-architecture/](https://www.plural.sh/blog/kubernetes-control-plane-architecture/)  
20. What is the Kubernetes API? \- Red Hat, 8월 27, 2025에 액세스, [https://www.redhat.com/en/topics/containers/what-is-the-kubernetes-api](https://www.redhat.com/en/topics/containers/what-is-the-kubernetes-api)  
21. What is the API Server in Kubernetes? Key Functions & Benefits \- Zesty, 8월 27, 2025에 액세스, [https://zesty.co/finops-glossary/k8s-api-server/](https://zesty.co/finops-glossary/k8s-api-server/)  
22. www.ibm.com, 8월 27, 2025에 액세스, [https://www.ibm.com/think/topics/etcd\#:\~:text=etcd%20is%20an%20open%20source,the%20popular%20container%20orchestration%20platform.](https://www.ibm.com/think/topics/etcd#:~:text=etcd%20is%20an%20open%20source,the%20popular%20container%20orchestration%20platform.)  
23. What is etcd? \- Red Hat, 8월 27, 2025에 액세스, [https://www.redhat.com/en/topics/containers/what-is-etcd](https://www.redhat.com/en/topics/containers/what-is-etcd)  
24. What Is etcd? | IBM, 8월 27, 2025에 액세스, [https://www.ibm.com/think/topics/etcd](https://www.ibm.com/think/topics/etcd)  
25. What is etcd in Kubernetes? \- ARMO, 8월 27, 2025에 액세스, [https://www.armosec.io/glossary/etcd-kubernetes/](https://www.armosec.io/glossary/etcd-kubernetes/)  
26. Kubernetes Scheduling: How It Works and Key Influencing Factors \- PerfectScale, 8월 27, 2025에 액세스, [https://www.perfectscale.io/blog/kubernetes-scheduling](https://www.perfectscale.io/blog/kubernetes-scheduling)  
27. Kubernetes Pod Scheduling: Tutorial and Best Practices \- CloudBolt, 8월 27, 2025에 액세스, [https://www.cloudbolt.io/kubernetes-pod-scheduling/](https://www.cloudbolt.io/kubernetes-pod-scheduling/)  
28. kube-scheduler \- Kubernetes, 8월 27, 2025에 액세스, [https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/)  
29. Kubernetes Scheduling: Understanding the Math Behind the Magic | by Roman Glushach, 8월 27, 2025에 액세스, [https://romanglushach.medium.com/kubernetes-scheduling-understanding-the-math-behind-the-magic-2305b57d45b1](https://romanglushach.medium.com/kubernetes-scheduling-understanding-the-math-behind-the-magic-2305b57d45b1)  
30. Kubernetes Controller Manager: A Gentle Introduction \- Komodor, 8월 27, 2025에 액세스, [https://komodor.com/learn/controller-manager/](https://komodor.com/learn/controller-manager/)  
31. Kubernetes Controller Manager: Role, Features, and Best Practices \- Zesty, 8월 27, 2025에 액세스, [https://zesty.co/finops-glossary/kubernetes-controller-manager/](https://zesty.co/finops-glossary/kubernetes-controller-manager/)  
32. Understanding Kube Controller Manager: The Heart of Kubernetes Resource Management, 8월 27, 2025에 액세스, [https://cloudcuddler.com/understanding-kube-controller-manager-kubernetes-resource-management/](https://cloudcuddler.com/understanding-kube-controller-manager-kubernetes-resource-management/)  
33. Core Kubernetes components \- Apptio, 8월 27, 2025에 액세스, [https://www.apptio.com/blog/kubernetes-components/](https://www.apptio.com/blog/kubernetes-components/)  
34. What Are Nodes in Kubernetes? Master & Worker Nodes Explained, 8월 27, 2025에 액세스, [https://zesty.co/finops-glossary/kubernetes-nodes/](https://zesty.co/finops-glossary/kubernetes-nodes/)  
35. Deep Dive into Kubernetes Components: Master, Worker, Node, Cluster, and More | by Santosh Kumar Perumal | Medium, 8월 27, 2025에 액세스, [https://medium.com/@santosh.personalid/deep-dive-into-kubernetes-components-master-worker-node-cluster-and-more-f0748b170d15](https://medium.com/@santosh.personalid/deep-dive-into-kubernetes-components-master-worker-node-cluster-and-more-f0748b170d15)  
36. The Technical History of Kubernetes | by Brian Grant \- ITNEXT, 8월 27, 2025에 액세스, [https://itnext.io/the-technical-history-of-kubernetes-2fe1988b522a](https://itnext.io/the-technical-history-of-kubernetes-2fe1988b522a)  
37. Kubernetes Deployment YAML File with Examples \- Spacelift, 8월 27, 2025에 액세스, [https://spacelift.io/blog/kubernetes-deployment-yaml](https://spacelift.io/blog/kubernetes-deployment-yaml)  
38. Deployments | Kubernetes, 8월 27, 2025에 액세스, [https://kubernetes.io/docs/concepts/workloads/controllers/deployment/](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/)  
39. Kubernetes deployment YAML Walkthrough | by MrDevSecOps \- Medium, 8월 27, 2025에 액세스, [https://medium.com/@mrdevsecops/kubernetes-deployment-yaml-walkthrough-267cc5b5dfe2](https://medium.com/@mrdevsecops/kubernetes-deployment-yaml-walkthrough-267cc5b5dfe2)  
40. Performing a Rolling Update | Kubernetes, 8월 27, 2025에 액세스, [https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/](https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/)  
41. Kubernetes ReplicaSets overview \- Sysdig, 8월 27, 2025에 액세스, [https://www.sysdig.com/learn-cloud-native/kubernetes-replicasets-overview](https://www.sysdig.com/learn-cloud-native/kubernetes-replicasets-overview)  
42. ReplicaSet \- Kubernetes, 8월 27, 2025에 액세스, [https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/](https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/)  
43. Kubernetes StatefulSet \- Examples & Best Practices \- vCluster, 8월 27, 2025에 액세스, [https://www.vcluster.com/blog/kubernetes-statefulset-examples-and-best-practices](https://www.vcluster.com/blog/kubernetes-statefulset-examples-and-best-practices)  
44. Guide to Kubernetes StatefulSets: Deploying Stateful Apps \- Plural.sh, 8월 27, 2025에 액세스, [https://www.plural.sh/blog/kubernetes-statefulset-guide/](https://www.plural.sh/blog/kubernetes-statefulset-guide/)  
45. StatefulSet Basics \- Kubernetes, 8월 27, 2025에 액세스, [https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/](https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/)  
46. StatefulSets | Kubernetes, 8월 27, 2025에 액세스, [https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/)  
47. DaemonSet | Kubernetes, 8월 27, 2025에 액세스, [https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/)  
48. Kubernetes Daemonset: A Practical Guide \- Spot.io, 8월 27, 2025에 액세스, [https://spot.io/resources/kubernetes-autoscaling/kubernetes-daemonset-a-practical-guide/](https://spot.io/resources/kubernetes-autoscaling/kubernetes-daemonset-a-practical-guide/)  
49. ConfigMaps | Kubernetes, 8월 27, 2025에 액세스, [https://kubernetes.io/docs/concepts/configuration/configmap/](https://kubernetes.io/docs/concepts/configuration/configmap/)  
50. Secrets | Kubernetes, 8월 27, 2025에 액세스, [https://kubernetes.io/docs/concepts/configuration/secret/](https://kubernetes.io/docs/concepts/configuration/secret/)  
51. Persistent Volumes | Kubernetes, 8월 27, 2025에 액세스, [https://kubernetes.io/docs/concepts/storage/persistent-volumes/](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)  
52. Kubernetes Persistent Volume Tutorial with PVCs \- Portworx, 8월 27, 2025에 액세스, [https://portworx.com/tutorial-kubernetes-persistent-volumes/](https://portworx.com/tutorial-kubernetes-persistent-volumes/)  
53. GKE Persistent volumes and dynamic provisioning | Google Kubernetes Engine (GKE), 8월 27, 2025에 액세스, [https://cloud.google.com/kubernetes-engine/docs/concepts/persistent-volumes](https://cloud.google.com/kubernetes-engine/docs/concepts/persistent-volumes)  
54. Role Based Access Control Good Practices \- Kubernetes, 8월 27, 2025에 액세스, [https://kubernetes.io/docs/concepts/security/rbac-good-practices/](https://kubernetes.io/docs/concepts/security/rbac-good-practices/)  
55. What is Argo CD? Overview & Tutorial \- Spacelift, 8월 27, 2025에 액세스, [https://spacelift.io/blog/argocd](https://spacelift.io/blog/argocd)  
56. What are Pod Security Standards? \- ARMO, 8월 27, 2025에 액세스, [https://www.armosec.io/glossary/pod-security-standards/](https://www.armosec.io/glossary/pod-security-standards/)  
57. Understanding Kubernetes Pod Security Standards \- Snyk, 8월 27, 2025에 액세스, [https://snyk.io/blog/understanding-kubernetes-pod-security-standards/](https://snyk.io/blog/understanding-kubernetes-pod-security-standards/)  
58. Pod Security Standards | Kubernetes, 8월 27, 2025에 액세스, [https://kubernetes.io/docs/concepts/security/pod-security-standards/](https://kubernetes.io/docs/concepts/security/pod-security-standards/)  
59. Understanding and applying Kubernetes Pod Security Policies \- Outshift | Cisco, 8월 27, 2025에 액세스, [https://outshift.cisco.com/blog/understanding-and-applying-kubernetes-pod-security-policy](https://outshift.cisco.com/blog/understanding-and-applying-kubernetes-pod-security-policy)  
60. Chapter 9: Pod Security Admission \- Kubernetes Guides \- Apptio, 8월 27, 2025에 액세스, [https://www.apptio.com/topics/kubernetes/best-practices/pod-security-admission/](https://www.apptio.com/topics/kubernetes/best-practices/pod-security-admission/)  
61. Understand Kubernetes Services | GKE networking \- Google Cloud, 8월 27, 2025에 액세스, [https://cloud.google.com/kubernetes-engine/docs/concepts/service](https://cloud.google.com/kubernetes-engine/docs/concepts/service)  
62. Using a Service to Expose Your App | Kubernetes, 8월 27, 2025에 액세스, [https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/](https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/)  
63. Service | Kubernetes, 8월 27, 2025에 액세스, [https://kubernetes.io/docs/concepts/services-networking/service/](https://kubernetes.io/docs/concepts/services-networking/service/)  
64. How to Create Deployments and Services in Kubernetes? \- ARMO, 8월 27, 2025에 액세스, [https://www.armosec.io/blog/kubernetes-deployment-and-service/](https://www.armosec.io/blog/kubernetes-deployment-and-service/)  
65. Ingress | Kubernetes, 8월 27, 2025에 액세스, [https://kubernetes.io/docs/concepts/services-networking/ingress/](https://kubernetes.io/docs/concepts/services-networking/ingress/)  
66. Ingress NGINX Controller for Kubernetes \- GitHub, 8월 27, 2025에 액세스, [https://github.com/kubernetes/ingress-nginx](https://github.com/kubernetes/ingress-nginx)  
67. Secure pod traffic with network policies \- Azure Kubernetes Service | Microsoft Learn, 8월 27, 2025에 액세스, [https://learn.microsoft.com/en-us/azure/aks/use-network-policies](https://learn.microsoft.com/en-us/azure/aks/use-network-policies)  
68. Network policy \- Calico Documentation \- Tigera, 8월 27, 2025에 액세스, [https://docs.tigera.io/calico/latest/reference/resources/networkpolicy](https://docs.tigera.io/calico/latest/reference/resources/networkpolicy)  
69. Network Policies | Kubernetes, 8월 27, 2025에 액세스, [https://kubernetes.io/docs/concepts/services-networking/network-policies/](https://kubernetes.io/docs/concepts/services-networking/network-policies/)  
70. Declare Network Policy \- Kubernetes, 8월 27, 2025에 액세스, [https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/](https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/)  
71. Kubernetes Network Policies Using Deny-All Default | Couchbase Docs, 8월 27, 2025에 액세스, [https://docs.couchbase.com/operator/current/tutorial-kubernetes-network-policy.html](https://docs.couchbase.com/operator/current/tutorial-kubernetes-network-policy.html)  
72. Enable a default deny policy for Kubernetes pods \- Calico Documentation \- Tigera, 8월 27, 2025에 액세스, [https://docs.tigera.io/calico/latest/network-policy/get-started/kubernetes-default-deny](https://docs.tigera.io/calico/latest/network-policy/get-started/kubernetes-default-deny)  
73. Add Network Policy \- Kyverno, 8월 27, 2025에 액세스, [https://kyverno.io/policies/best-practices/add-network-policy/add-network-policy/](https://kyverno.io/policies/best-practices/add-network-policy/add-network-policy/)  
74. networkpolicy/tutorial: Kubernetes Network Policy Tutorial \- GitHub, 8월 27, 2025에 액세스, [https://github.com/networkpolicy/tutorial](https://github.com/networkpolicy/tutorial)  
75. The Single-Node Kubernetes Showdown: minikube vs. kind vs. k3d \- Oilbeater, 8월 27, 2025에 액세스, [https://oilbeater.com/en/2024/02/22/minikube-vs-kind-vs-k3d/](https://oilbeater.com/en/2024/02/22/minikube-vs-kind-vs-k3d/)  
76. www.sysdig.com, 8월 27, 2025에 액세스, [https://www.sysdig.com/learn-cloud-native/what-is-minikube\#:\~:text=Minikube%20works%20by%20setting%20up,of%20%C3%A2%E2%82%AC%C5%93drivers.](https://www.sysdig.com/learn-cloud-native/what-is-minikube#:~:text=Minikube%20works%20by%20setting%20up,of%20%C3%A2%E2%82%AC%C5%93drivers.)  
77. Getting started with Minikube: a Minikube tutorial \- Sensu, 8월 27, 2025에 액세스, [https://sensu.io/blog/minikube-tutorial](https://sensu.io/blog/minikube-tutorial)  
78. Minikube vs Kind: A Comprehensive Comparison | Better Stack Community, 8월 27, 2025에 액세스, [https://betterstack.com/community/guides/scaling-docker/minikube-vs-kubernetes/](https://betterstack.com/community/guides/scaling-docker/minikube-vs-kubernetes/)  
79. Minikube vs. Kind vs. K3s \- DevZero, 8월 27, 2025에 액세스, [https://www.devzero.io/blog/minikube-vs-kind-vs-k3s](https://www.devzero.io/blog/minikube-vs-kind-vs-k3s)  
80. kind.sigs.k8s.io, 8월 27, 2025에 액세스, [https://kind.sigs.k8s.io/docs/user/quick-start/\#:\~:text=kind%20runs%20a%20local%20Kubernetes,to%20run%20in%20a%20container.](https://kind.sigs.k8s.io/docs/user/quick-start/#:~:text=kind%20runs%20a%20local%20Kubernetes,to%20run%20in%20a%20container.)  
81. kind \- Kubernetes, 8월 27, 2025에 액세스, [https://kind.sigs.k8s.io/](https://kind.sigs.k8s.io/)  
82. How To Use Kind To Deploy Kubernetes Clusters? \- GeeksforGeeks, 8월 27, 2025에 액세스, [https://www.geeksforgeeks.org/devops/how-to-use-kind-to-deploy-kubernetes-clusters/](https://www.geeksforgeeks.org/devops/how-to-use-kind-to-deploy-kubernetes-clusters/)  
83. Exploring Kubernetes 1.29 with Kind | by Gineesh Madapparambath | techbeatly \- Medium, 8월 27, 2025에 액세스, [https://medium.com/techbeatly/exploring-kubernetes-1-29-with-kind-a2704e1c729d](https://medium.com/techbeatly/exploring-kubernetes-1-29-with-kind-a2704e1c729d)  
84. Build a Docker Image and Push to Docker Hub \- STACKSIMPLIFY, 8월 27, 2025에 액세스, [https://www.stacksimplify.com/aws-eks/docker-basics/build-docker-image/](https://www.stacksimplify.com/aws-eks/docker-basics/build-docker-image/)  
85. Publishing Images to Docker Hub \- GeeksforGeeks, 8월 27, 2025에 액세스, [https://www.geeksforgeeks.org/devops/docker-publishing-images-to-docker-hub/](https://www.geeksforgeeks.org/devops/docker-publishing-images-to-docker-hub/)  
86. Prometheus Monitoring for Kubernetes Cluster \[Tutorial\] \- Spacelift, 8월 27, 2025에 액세스, [https://spacelift.io/blog/prometheus-kubernetes](https://spacelift.io/blog/prometheus-kubernetes)  
87. Kubernetes Monitoring with Grafana, 8월 27, 2025에 액세스, [https://grafana.com/solutions/kubernetes/](https://grafana.com/solutions/kubernetes/)  
88. Get started with Grafana and Prometheus, 8월 27, 2025에 액세스, [https://grafana.com/docs/grafana/latest/getting-started/get-started-grafana-prometheus/](https://grafana.com/docs/grafana/latest/getting-started/get-started-grafana-prometheus/)  
89. Monitoring Kubernetes with Prometheus and Grafana on Amazon EKS | by Prasad Midde, 8월 27, 2025에 액세스, [https://medium.com/@prasad.midde3/monitoring-kubernetes-with-prometheus-and-grafana-on-amazon-eks-18f2371f2597](https://medium.com/@prasad.midde3/monitoring-kubernetes-with-prometheus-and-grafana-on-amazon-eks-18f2371f2597)  
90. Prometheus Operator creates/configures/manages Prometheus clusters atop Kubernetes \- GitHub, 8월 27, 2025에 액세스, [https://github.com/prometheus-operator/prometheus-operator](https://github.com/prometheus-operator/prometheus-operator)  
91. Kubernetes Operator User's Guide \- Oracle Help Center, 8월 27, 2025에 액세스, [https://docs.oracle.com/en/database/other-databases/timesten/18.1/kubernetes-operator/prometheus-operator.html](https://docs.oracle.com/en/database/other-databases/timesten/18.1/kubernetes-operator/prometheus-operator.html)  
92. Getting Started With Prometheus Operator \- OpsRamp, 8월 27, 2025에 액세스, [https://www.opsramp.com/guides/prometheus-monitoring/prometheus-operator/](https://www.opsramp.com/guides/prometheus-monitoring/prometheus-operator/)  
93. Getting Started \- Prometheus Operator, 8월 27, 2025에 액세스, [https://prometheus-operator.dev/docs/developer/getting-started/](https://prometheus-operator.dev/docs/developer/getting-started/)  
94. Set up a scalable EFK/ELK stack on Kubernetes: Your In-House ..., 8월 27, 2025에 액세스, [https://blog.devops.dev/set-up-a-scalable-efk-elk-stack-on-kubernetes-your-in-house-logging-solution-cac5aa38b919](https://blog.devops.dev/set-up-a-scalable-efk-elk-stack-on-kubernetes-your-in-house-logging-solution-cac5aa38b919)  
95. EFK vs ELK: Understanding the Key Differences in Logging and Monitoring Stacks, 8월 27, 2025에 액세스, [https://charleswan111.medium.com/efk-vs-elk-understanding-the-key-differences-in-logging-and-monitoring-stacks-c299ea11c310](https://charleswan111.medium.com/efk-vs-elk-understanding-the-key-differences-in-logging-and-monitoring-stacks-c299ea11c310)  
96. Logging in Kubernetes: EFK vs PLG Stack \- InfraCloud, 8월 27, 2025에 액세스, [https://www.infracloud.io/blogs/logging-in-kubernetes-efk-vs-plg-stack/](https://www.infracloud.io/blogs/logging-in-kubernetes-efk-vs-plg-stack/)  
97. Kubernetes Logging and Monitoring: The Elasticsearch, Fluentd, and Kibana (EFK) Stack \- Part 1 \- Platform9, 8월 27, 2025에 액세스, [https://platform9.com/blog/kubernetes-logging-and-monitoring-the-elasticsearch-fluentd-and-kibana-efk-stack-part-1-fluentd-architecture-and-configuration/](https://platform9.com/blog/kubernetes-logging-and-monitoring-the-elasticsearch-fluentd-and-kibana-efk-stack-part-1-fluentd-architecture-and-configuration/)  
98. Application Logging in Kubernetes with fluentd | by Rosemary Wang \- Medium, 8월 27, 2025에 액세스, [https://medium.com/@joatmon08/application-logging-in-kubernetes-with-fluentd-4556f1573672](https://medium.com/@joatmon08/application-logging-in-kubernetes-with-fluentd-4556f1573672)  
99. Kubernetes \- Fluentd, 8월 27, 2025에 액세스, [https://docs.fluentd.org/container-deployment/kubernetes](https://docs.fluentd.org/container-deployment/kubernetes)  
100. Elasticsearch on Kubernetes: Deploy & Scale Easily \- Portworx, 8월 27, 2025에 액세스, [https://portworx.com/elasticsearch-kubernetes/](https://portworx.com/elasticsearch-kubernetes/)  
101. helm.sh, 8월 27, 2025에 액세스, [https://helm.sh/\#:\~:text=Helm%20helps%20you%20manage%20Kubernetes,the%20copy%2Dand%2Dpaste.](https://helm.sh/#:~:text=Helm%20helps%20you%20manage%20Kubernetes,the%20copy%2Dand%2Dpaste.)  
102. What is Helm in Kubernetes? | IBM, 8월 27, 2025에 액세스, [https://www.ibm.com/think/topics/helm](https://www.ibm.com/think/topics/helm)  
103. What is Helm? \- Red Hat, 8월 27, 2025에 액세스, [https://www.redhat.com/en/topics/devops/what-is-helm](https://www.redhat.com/en/topics/devops/what-is-helm)  
104. Helm, 8월 27, 2025에 액세스, [https://helm.sh/](https://helm.sh/)  
105. Getting Started \- Helm, 8월 27, 2025에 액세스, [https://helm.sh/docs/chart\_template\_guide/getting\_started/](https://helm.sh/docs/chart_template_guide/getting_started/)  
106. Helm Chart Tutorial: A Complete Guide \- Middleware, 8월 27, 2025에 액세스, [https://middleware.io/blog/helm-chart-tutorial/](https://middleware.io/blog/helm-chart-tutorial/)  
107. Understand a Helm chart structure \- Bitnami Documentation, 8월 27, 2025에 액세스, [https://docs.bitnami.com/kubernetes/faq/administration/understand-helm-chart/](https://docs.bitnami.com/kubernetes/faq/administration/understand-helm-chart/)  
108. Charts \- Helm, 8월 27, 2025에 액세스, [https://helm.sh/docs/topics/charts/](https://helm.sh/docs/topics/charts/)  
109. Linkerd vs Istio, a service mesh comparison \- Buoyant.io, 8월 27, 2025에 액세스, [https://www.buoyant.io/linkerd-vs-istio](https://www.buoyant.io/linkerd-vs-istio)  
110. CI/CD with Jenkins in 3 Steps | Codefresh, 8월 27, 2025에 액세스, [https://codefresh.io/learn/jenkins/ci-cd-with-jenkins-in-3-steps/](https://codefresh.io/learn/jenkins/ci-cd-with-jenkins-in-3-steps/)  
111. Day 25: Kubernetes CI/CD Pipelines | by Vinoth Subbiah | Medium, 8월 27, 2025에 액세스, [https://medium.com/@vinoji2005/day-25-kubernetes-ci-cd-pipelines-a432dfdb6e96](https://medium.com/@vinoji2005/day-25-kubernetes-ci-cd-pipelines-a432dfdb6e96)  
112. codefresh.io, 8월 27, 2025에 액세스, [https://codefresh.io/learn/argo-cd/\#:\~:text=GitOps%20agent%E2%80%94Argo%20CD%20is,application%20updates%20in%20one%20system.](https://codefresh.io/learn/argo-cd/#:~:text=GitOps%20agent%E2%80%94Argo%20CD%20is,application%20updates%20in%20one%20system.)  
113. Argo CD, 8월 27, 2025에 액세스, [https://argoproj.github.io/cd/](https://argoproj.github.io/cd/)  
114. Understanding Argo CD: Kubernetes GitOps Made Simple \- Codefresh, 8월 27, 2025에 액세스, [https://codefresh.io/learn/argo-cd/](https://codefresh.io/learn/argo-cd/)  
115. What is a service mesh? | Linkerd, 8월 27, 2025에 액세스, [https://linkerd.io/what-is-a-service-mesh/](https://linkerd.io/what-is-a-service-mesh/)  
116. Linkerd vs. Istio: Comparison for Kubernetes Service Mesh \- overcast blog, 8월 27, 2025에 액세스, [https://overcast.blog/linkerd-vs-istio-comparison-for-kubernetes-service-mesh-7e3c5dfab84f](https://overcast.blog/linkerd-vs-istio-comparison-for-kubernetes-service-mesh-7e3c5dfab84f)  
117. Linkerd vs. Istio: 7 Key Differences \- Solo.io, 8월 27, 2025에 액세스, [https://www.solo.io/topics/istio/linkerd-vs-istio](https://www.solo.io/topics/istio/linkerd-vs-istio)  
118. What is Istio? \- Red Hat, 8월 27, 2025에 액세스, [https://www.redhat.com/en/topics/microservices/what-is-istio](https://www.redhat.com/en/topics/microservices/what-is-istio)  
119. Istio Architecture: 4 Key Components, Multi-Cluster and More | Solo.io, 8월 27, 2025에 액세스, [https://www.solo.io/topics/istio/istio-architecture](https://www.solo.io/topics/istio/istio-architecture)  
120. Istio / Traffic Management, 8월 27, 2025에 액세스, [https://istio.io/latest/docs/concepts/traffic-management/](https://istio.io/latest/docs/concepts/traffic-management/)  
121. Istio / Virtual Service, 8월 27, 2025에 액세스, [https://istio.io/latest/docs/reference/config/networking/virtual-service/](https://istio.io/latest/docs/reference/config/networking/virtual-service/)  
122. Destination Rule \- Istio, 8월 27, 2025에 액세스, [https://istio.io/latest/docs/reference/config/networking/destination-rule/](https://istio.io/latest/docs/reference/config/networking/destination-rule/)  
123. www.ibm.com, 8월 27, 2025에 액세스, [https://www.ibm.com/think/topics/knative\#:\~:text=Knative%20enables%20serverless%20workloads%20to,the%20Kubernetes%20container%20orchestration%20platform.](https://www.ibm.com/think/topics/knative#:~:text=Knative%20enables%20serverless%20workloads%20to,the%20Kubernetes%20container%20orchestration%20platform.)  
124. What is Knative? \- Red Hat, 8월 27, 2025에 액세스, [https://www.redhat.com/en/topics/microservices/what-is-knative](https://www.redhat.com/en/topics/microservices/what-is-knative)  
125. What I learned about Kubernetes and Knative Serverless \- Red Hat, 8월 27, 2025에 액세스, [https://www.redhat.com/en/blog/what-i-learned-about-kubernetes-and-knative-serverless](https://www.redhat.com/en/blog/what-i-learned-about-kubernetes-and-knative-serverless)  
126. Overview \- Knative, 8월 27, 2025에 액세스, [https://knative.dev/docs/serving/](https://knative.dev/docs/serving/)  
127. Google Kubernetes Engine: The Ultimate Guide \- Plural.sh, 8월 27, 2025에 액세스, [https://www.plural.sh/blog/google-kubernetes-engine-guide/](https://www.plural.sh/blog/google-kubernetes-engine-guide/)  
128. Azure Kubernetes Service: Features, architecture, and getting started \- Spot.io, 8월 27, 2025에 액세스, [https://spot.io/resources/azure-kubernetes-service/azure-kubernetes-service-features-architecture-getting-started/](https://spot.io/resources/azure-kubernetes-service/azure-kubernetes-service-features-architecture-getting-started/)  
129. What is Azure Kubernetes Service (AKS)?, 8월 27, 2025에 액세스, [https://learn.microsoft.com/en-us/azure/aks/what-is-aks](https://learn.microsoft.com/en-us/azure/aks/what-is-aks)  
130. Managed Kubernetes Service \- Amazon EKS Features \- AWS, 8월 27, 2025에 액세스, [https://aws.amazon.com/eks/features/](https://aws.amazon.com/eks/features/)  
131. AWS EKS: 12 Key Features and 4 Deployment Options, 8월 27, 2025에 액세스, [https://bluexp.netapp.com/blog/aws-cvo-blg-aws-eks-12-key-features-and-4-deployment-options](https://bluexp.netapp.com/blog/aws-cvo-blg-aws-eks-12-key-features-and-4-deployment-options)  
132. AWS EKS Architecture Overview: Key Components and Best Practices, 8월 27, 2025에 액세스, [https://lumigo.io/aws-eks/aws-eks-architecture-overview-key-components-and-best-practices/](https://lumigo.io/aws-eks/aws-eks-architecture-overview-key-components-and-best-practices/)  
133. GKE vs EKS: Which is Most Commonly Used in Production and What Are Their Key Advantages? : r/kubernetes \- Reddit, 8월 27, 2025에 액세스, [https://www.reddit.com/r/kubernetes/comments/1irr7x4/gke\_vs\_eks\_which\_is\_most\_commonly\_used\_in/](https://www.reddit.com/r/kubernetes/comments/1irr7x4/gke_vs_eks_which_is_most_commonly_used_in/)  
134. GKE overview | Google Kubernetes Engine (GKE) | Google Cloud, 8월 27, 2025에 액세스, [https://cloud.google.com/kubernetes-engine/docs/concepts/kubernetes-engine-overview](https://cloud.google.com/kubernetes-engine/docs/concepts/kubernetes-engine-overview)  
135. Google Kubernetes Engine: Architecture, Pricing & Best Practices | Spot.io, 8월 27, 2025에 액세스, [https://spot.io/resources/google-kubernetes-engine/google-kubernetes-engine-architecture-pricing-best-practices/](https://spot.io/resources/google-kubernetes-engine/google-kubernetes-engine-architecture-pricing-best-practices/)  
136. What is the Relationship Between Kubernetes and Google Kubernetes Engine?, 8월 27, 2025에 액세스, [https://www.geeksforgeeks.org/system-design/what-is-the-relationship-between-kubernetes-and-google-kubernetes-engine/](https://www.geeksforgeeks.org/system-design/what-is-the-relationship-between-kubernetes-and-google-kubernetes-engine/)  
137. Google Kubernetes Engine: 5 Key Features and Getting Started \- Aqua Security, 8월 27, 2025에 액세스, [https://www.aquasec.com/cloud-native-academy/container-platforms/google-kubernetes-engine/](https://www.aquasec.com/cloud-native-academy/container-platforms/google-kubernetes-engine/)  
138. EKS vs AKS vs GKE: 5 Critical Differences \- SentinelOne, 8월 27, 2025에 액세스, [https://www.sentinelone.com/cybersecurity-101/cybersecurity/eks-vs-aks-vs-gke/](https://www.sentinelone.com/cybersecurity-101/cybersecurity/eks-vs-aks-vs-gke/)  
139. MicroK8s vs k3s vs Minikube, 8월 27, 2025에 액세스, [https://microk8s.io/compare](https://microk8s.io/compare)  
140. EKS vs. AKS vs. GKE: Choosing the Ideal Kubernetes Platform \- Veritis, 8월 27, 2025에 액세스, [https://www.veritis.com/blog/eks-vs-aks-vs-gke-which-is-the-right-kubernetes-platform-for-you/](https://www.veritis.com/blog/eks-vs-aks-vs-gke-which-is-the-right-kubernetes-platform-for-you/)  
141. EKS vs. GKE Networking \- Jason Umiker \- Medium, 8월 27, 2025에 액세스, [https://jason-umiker.medium.com/eks-vs-gke-networking-e1dd397fe86d](https://jason-umiker.medium.com/eks-vs-gke-networking-e1dd397fe86d)  
142. EKS vs. AKS: A Deep Dive into Technical Differences | by cyber\_pix | Medium, 8월 27, 2025에 액세스, [https://medium.com/@use.abhiram/eks-vs-aks-a-deep-dive-into-technical-differences-71f8dd09d9a0](https://medium.com/@use.abhiram/eks-vs-aks-a-deep-dive-into-technical-differences-71f8dd09d9a0)  
143. GKE vs AKS vs EKS \- Hasura, 8월 27, 2025에 액세스, [https://hasura.io/blog/gke-vs-aks-vs-eks-411f080640dc](https://hasura.io/blog/gke-vs-aks-vs-eks-411f080640dc)  
144. Kubernetes Cost: EKS vs AKS vs GKE \- Sedai, 8월 27, 2025에 액세스, [https://www.sedai.io/blog/kubernetes-cost-eks-vs-aks-vs-gke](https://www.sedai.io/blog/kubernetes-cost-eks-vs-aks-vs-gke)  
145. Serverless Kubernetes costs for EKS, AKS, GKE, and OKE \- Oracle Blogs, 8월 27, 2025에 액세스, [https://blogs.oracle.com/cloud-infrastructure/post/serverless-kubernetes-costs-eks-aks-gke-oke](https://blogs.oracle.com/cloud-infrastructure/post/serverless-kubernetes-costs-eks-aks-gke-oke)